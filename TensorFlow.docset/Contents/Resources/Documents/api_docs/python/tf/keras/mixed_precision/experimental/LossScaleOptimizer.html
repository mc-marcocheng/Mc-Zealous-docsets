<!DOCTYPE html><html dir="ltr" lang="en"><head>
<meta content="157101835696-ooapojlodmuabs2do2vuhhnf90bccmoi.apps.googleusercontent.com" name="google-signin-client-id"/>
<meta content="profile email" name="google-signin-scope"/>
<meta content="TensorFlow" property="og:site_name"/>
<meta content="website" property="og:type"/>
<meta content="#ff6f00" name="theme-color"/>
<meta charset="utf-8"/>
<meta content="IE=Edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<link crossorigin="use-credentials" href="_pwa/tensorflow/manifest.json" rel="manifest"/>
<link crossorigin="" href="/www.gstatic.com" rel="preconnect"/>
<link crossorigin="" href="/fonts.gstatic.com" rel="preconnect"/>
<link crossorigin="" href="/fonts.googleapis.com" rel="preconnect"/>
<link href="../../../../../../main.css" rel="stylesheet"/>

<noscript>

</noscript>
<link href="https://www.gstatic.com/devrel-devsite/prod/v3e2dbdc40e7394635e5230ecc02cb28039ea55a5d72db9939d2fb9fc9e16d0ff/tensorflow/images/favicon.png" rel="shortcut icon"/>
<link href="https://www.gstatic.com/devrel-devsite/prod/v3e2dbdc40e7394635e5230ecc02cb28039ea55a5d72db9939d2fb9fc9e16d0ff/tensorflow/images/apple-touch-icon-180x180.png" rel="apple-touch-icon"/><link href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer" rel="canonical"/><link href="https://www.tensorflow.org/s/opensearch.xml" rel="search" title="TensorFlow" type="application/opensearchdescription+xml"/>
<title>tf.keras.mixed_precision.experimental.LossScaleOptimizer</title>
<meta content="tf.keras.mixed_precision.experimental.LossScaleOptimizer" property="og:title"/>
<meta content="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer" property="og:url"/>
<meta content="en" property="og:locale"/>

</head>
<body class="" layout="docs" pending="" theme="tensorflow-theme" type="reference">
<devsite-progress id="app-progress" type="indeterminate"></devsite-progress>
<section class="devsite-wrapper"> <devsite-book-nav scrollbars="">

</devsite-book-nav>
<section id="gc-wrapper">
<main class="devsite-main-content" has-book-nav="" has-toc="" role="main">
<devsite-toc class="devsite-nav"></devsite-toc>
<devsite-content>
<article class="devsite-article">
<article class="devsite-article-inner"><style>
        /* Styles inlined from /site-assets/css/style.css */
/* override theme */
table img {
  max-width: 100%;
}

/* override var element to differentiate color from comment */
var, var code, var span, .prettyprint var span {
  color: #039be5;
}

/* .devsite-terminal virtualenv prompt */
.tfo-terminal-venv::before {
  content: "(venv) $ " !important;
}

/* .devsite-terminal root prompt */
.tfo-terminal-root::before {
  content: "# " !important;
}

/* .devsite-terminal Windows prompt */
.tfo-terminal-windows::before {
  content: "C:\\> " !important;
}

/* .devsite-terminal Windows prompt w/ virtualenv */
.tfo-terminal-windows-venv::before {
  content: "(venv) C:\\> " !important;
}

.tfo-diff-green-one-level + * {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-green + * > * {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-green-list + ul > li:first-of-type {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-red-one-level + * {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

.tfo-diff-red + * > * {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

.tfo-diff-red-list + ul > li:first-of-type {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

devsite-code .tfo-notebook-code-cell-output {
  max-height: 300px;
  overflow: auto;
  background: rgba(255, 247, 237, 1);  /* orange bg to distinguish from input code cells */
}

devsite-code .tfo-notebook-code-cell-output + .devsite-code-buttons-container button {
  background: rgba(255, 247, 237, .7);  /* orange bg to distinguish from input code cells */
}

devsite-code[dark-code] .tfo-notebook-code-cell-output {
  background: rgba(64, 78, 103, 1);  /* medium slate */
}

devsite-code[dark-code] .tfo-notebook-code-cell-output + .devsite-code-buttons-container button {
  background: rgba(64, 78, 103, .7);  /* medium slate */
}

/* override default table styles for notebook buttons */
.devsite-table-wrapper .tfo-notebook-buttons {
  display: inline-block;
  margin-left: 3px;
  width: auto;
}

.tfo-notebook-buttons td {
  padding-left: 0;
  padding-right: 20px;
}

.tfo-notebook-buttons a,
.tfo-notebook-buttons :link,
.tfo-notebook-buttons :visited {
  border-radius: 8px;
  box-shadow: 0 1px 2px 0 rgba(60, 64, 67, .3), 0 1px 3px 1px rgba(60, 64, 67, .15);
  color: #202124;
  padding: 12px 24px;
  transition: box-shadow 0.2s;
}

.tfo-notebook-buttons a:hover,
.tfo-notebook-buttons a:focus {
  box-shadow: 0 1px 2px 0 rgba(60, 64, 67, .3), 0 2px 6px 2px rgba(60, 64, 67, .15);
}

.tfo-notebook-buttons tr {
  background: 0;
  border: 0;
}

/* on rendered notebook page,
   remove link to webpage since we're already here */
.tfo-notebook-buttons:not(.tfo-api) td:first-child {
  display: none;
}

.tfo-notebook-buttons td > a {
  -webkit-box-align: center;
  -ms-flex-align: center;
  align-items: center;
  display: -webkit-box;
  display: -ms-flexbox;
  display: flex;
}

.tfo-notebook-buttons td > a > img {
  margin-right: 8px;
}

/* landing pages */

.tfo-landing-row-item-inset-white {
  background-color: #fff;
  padding: 32px;
}

.tfo-landing-row-item-inset-white ol,
.tfo-landing-row-item-inset-white ul {
  padding-left: 20px;
}

/* colab callout button */
.colab-callout-row devsite-code {
  border-radius: 8px 8px 0 0;
  box-shadow: none;
}

.colab-callout-footer {
  background: #e3e4e7;
  border-radius: 0 0 8px 8px;
  color: #37474f;
  padding: 20px;
}

.colab-callout-row devsite-code[dark-code] + .colab-callout-footer {
  background: #3f4f66;
}


.colab-callout-footer > .button {
  margin-top: 4px;
  color: #ff5c00;
}

.colab-callout-footer > a > span {
  padding-top: 10px;
  vertical-align: middle;
  color: #37474f;
  padding-left: 10px;
  padding-right: 10px;
  font-size: 14px;
}

.colab-callout-row devsite-code[dark-code] + .colab-callout-footer > a > span {
  color: #fff;
}

a.colab-button {
  background: rgba(255, 255, 255, .75);
  border: solid 1px rgba(0, 0, 0, .08);
  border-bottom-color: rgba(0, 0, 0, .15);
  border-radius: 4px;
  color: #aaa;
  display: inline-block;
  font-size: 11px !important;
  font-weight: 300;
  line-height: 16px;
  padding: 4px 8px;
  text-decoration: none;
  text-transform: uppercase;
}

a.colab-button:hover {
  background: white;
  border-color: rgba(0, 0, 0, .2);
  color: #666;
}

a.colab-button span {
  background: url(/images/colab_logo_button.svg) no-repeat 1px 1px / 20px;
  border-radius: 4px;
  display: inline-block;
  padding-left: 24px;
  text-decoration: none;
}

@media screen and (max-width: 600px) {
  .tfo-notebook-buttons td {
    display: block;
  }
}

/* guide and tutorials landing page cards and sections */

.tfo-landing-page-card {
  padding: 16px;
  box-shadow: 0 0 36px rgba(0,0,0,0.1);
  border-radius: 10px;
}

/* Page section headings */
.tfo-landing-page-heading h2, h2.tfo-landing-page-heading {
  font-family: "Google Sans", sans-serif;
  color: #425066;
  font-size: 30px;
  font-weight: 700;
  line-height: 40px;
}

/* Item title headings */
.tfo-landing-page-heading h3, h3.tfo-landing-page-heading,
.tfo-landing-page-card h3, h3.tfo-landing-page-card {
  font-family: "Google Sans", sans-serif;
  color: #425066;
  font-size: 20px;
  font-weight: 500;
  line-height: 26px;
}

/* List of tutorials notebooks for subsites */
.tfo-landing-page-resources-ul {
  padding-left: 15px
}

.tfo-landing-page-resources-ul > li {
  margin: 6px 0;
}

/* Temporary fix to hide product description in header on landing pages */
devsite-header .devsite-product-description {
  display: none;
}

        </style> <div class="devsite-banner devsite-banner-announcement">
<div class="devsite-banner-message">
<div class="devsite-banner-message-text">
            Missed TensorFlow Dev Summit? Check out the video playlist. <a class="button button-primary button-tfo-announcement" href="https://goo.gle/TFDS20AllSessions">Watch recordings</a>
</div>
</div>
</div>
<div class="devsite-article-meta">
<ul class="devsite-breadcrumb-list">
<li class="devsite-breadcrumb-item">
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="1" href="">
            TensorFlow
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="2" href="api">
            API
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="3" href="api_docs">
            TensorFlow Core v2.1.0
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="4" href="api_docs/python/tf">
            Python
      
  </a>
</li>
</ul>
<devsite-page-rating hover-rating-star="0" position="header" selected-rating="0">
</devsite-page-rating>
</div>
<a class="dashingAutolink" name="autolink-2029"></a><a class="dashAnchor" name="//apple_ref/cpp/Function/tf.keras.mixed_precision.experimental.LossScaleOptimizer"></a><h1 class="dash-function">tf.keras.mixed_precision.experimental.LossScaleOptimizer</h1>
<devsite-toc class="devsite-nav" devsite-toc-embedded="">
</devsite-toc>
<div class="devsite-article-body clearfix">
<p></p>
<!-- DO NOT EDIT! Automatically generated file. -->
<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<meta content="tf.keras.mixed_precision.experimental.LossScaleOptimizer" itemprop="name"/>
<meta content="Stable" itemprop="path"/>
<meta content="__init__" itemprop="property"/>
<meta content="add_slot" itemprop="property"/>
<meta content="add_weight" itemprop="property"/>
<meta content="apply_gradients" itemprop="property"/>
<meta content="from_config" itemprop="property"/>
<meta content="get_config" itemprop="property"/>
<meta content="get_gradients" itemprop="property"/>
<meta content="get_scaled_loss" itemprop="property"/>
<meta content="get_slot" itemprop="property"/>
<meta content="get_slot_names" itemprop="property"/>
<meta content="get_unscaled_gradients" itemprop="property"/>
<meta content="get_updates" itemprop="property"/>
<meta content="get_weights" itemprop="property"/>
<meta content="minimize" itemprop="property"/>
<meta content="set_weights" itemprop="property"/>
<meta content="variables" itemprop="property"/>
</div>
<p><devsite-nav-buttons name="version" param="reset">
<button default="" value="stable">See Stable</button>
<button value="nightly">See Nightly</button>
</devsite-nav-buttons></p>
<!-- Stable -->
<table align="left" class="tfo-notebook-buttons tfo-api">
<tbody><tr><td>
<a href="versions/r1.15/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer" target="_blank">
<img src="https://www.tensorflow.org/images/tf_logo_32px.png"/>
  TensorFlow 1 version</a>
</td>
<td>
<a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L49-L333" target="_blank">
<img src="https://www.tensorflow.org/images/GitHub-Mark-32px.png"/>
    View source on GitHub
  </a>
</td></tr></tbody></table>
<p>An optimizer that applies loss scaling.</p>
<p>Inherits From: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer"><code dir="ltr" translate="no">Optimizer</code></a></p>
<section class="expandable">
<h4 class="showalways">View aliases</h4>
<p>
<b>Compat aliases for migration</b>
</p><p>See
<a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for
more details.</p>
<p><a href="api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer"><code dir="ltr" translate="no">tf.compat.v1.keras.mixed_precision.experimental.LossScaleOptimizer</code></a></p>
</section>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">tf.keras.mixed_precision.experimental.LossScaleOptimizer(
    optimizer, loss_scale
)
</code></pre>
<h3>Used in the notebooks</h3>
<table class="vertical-rules">
<thead>
<tr>
<th>Used in the guide</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<ul>
<li><a href="https://www.tensorflow.org/guide/keras/mixed_precision">Mixed precision</a></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Loss scaling is a process that multiplies the loss by a multiplier called the
loss scale, and divides each gradient by the same multiplier. The pseudocode
for this process is:</p>
<pre class="prettyprint" dir="ltr" translate="no"><code dir="ltr" translate="no">loss = ...
loss *= loss_scale
grads = gradients(loss, vars)
grads /= loss_scale
</code></pre>
<p>Mathematically, loss scaling has no effect, but can help avoid numerical
underflow in intermediate gradients when float16 tensors are used. By
multiplying the loss, each intermediate gradient will have the same multiplier
applied.</p>
<p>The loss scale can either be a fixed constant, chosen by the user, or be
dynamically determined. Dynamically determining the loss scale is convenient
as a loss scale does not have to be explicitly chosen. However it reduces
performance.</p>
<p>This optimizer wraps another optimizer and applies loss scaling to it via a
<code dir="ltr" translate="no">LossScale</code>. Loss scaling is applied whenever gradients are
computed, either through <code dir="ltr" translate="no">minimize()</code> or <code dir="ltr" translate="no">get_gradients()</code>. The loss scale is
updated via <a href="https://www.tensorflow.org/api_docs/python/tf/mixed_precision/experimental/LossScale#update"><code dir="ltr" translate="no">LossScale.update()</code></a> whenever gradients are applied, either
through <code dir="ltr" translate="no">minimize()</code> or <code dir="ltr" translate="no">apply_gradients()</code>. For example:</p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">opt = tf.keras.optimizers.SGD(0.1)
opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, &#34;dynamic&#34;)
# &#39;minimize&#39; applies loss scaling to the loss and updates the loss sale.
opt.minimize(loss_fn)
</code></pre>
<p>If a <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code dir="ltr" translate="no">tf.GradientTape</code></a> is used to compute gradients instead of
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#minimize"><code dir="ltr" translate="no">LossScaleOptimizer.minimize</code></a> or <a href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer#get_gradients"><code dir="ltr" translate="no">LossScaleOptimizer.get_gradients</code></a>, the loss
and gradients must be scaled manually. This can be done by calling
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer#get_scaled_loss"><code dir="ltr" translate="no">LossScaleOptimizer.get_scaled_loss</code></a> before passing the loss to
<a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code dir="ltr" translate="no">tf.GradientTape</code></a>, and <a href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer#get_unscaled_gradients"><code dir="ltr" translate="no">LossScaleOptimizer.get_unscaled_gradients</code></a> after
computing the gradients with <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code dir="ltr" translate="no">tf.GradientTape</code></a>. For example:</p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(...)
vars = ...
with tf.GradientTape() as tape:
  loss = ...
  scaled_loss = opt.get_scaled_loss(loss)
scaled_grads = tape.gradient(scaled_loss, vars)
grads = opt.get_unscaled_gradients(scaled_grads)
opt.apply_gradients(zip(grads, vars))  # Loss scale will be updated here
</code></pre>
<h4 id="args_6">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">optimizer</code></b>: The Optimizer instance to wrap.</li>
<li><b><code dir="ltr" translate="no">loss_scale</code></b>: The loss scale to scale the loss and gradients. This can
either be an int/float to use a fixed loss scale, the string &#34;dynamic&#34;
to use dynamic loss scaling, or an instance of a LossScale. The string
&#34;dynamic&#34; equivalent to passing <code dir="ltr" translate="no">DynamicLossScale()</code>, and passing an
int/float is equivalent to passing a FixedLossScale with the given loss
scale.</li>
</ul>
<h4 id="attributes_2">Attributes:</h4>
<ul>
<li><b><code dir="ltr" translate="no">iterations</code></b>:   Variable. The number of training steps this Optimizer has run.</li>
<li><b><code dir="ltr" translate="no">learning_rate</code></b></li>
<li><b><code dir="ltr" translate="no">loss_scale</code></b>:   The <code dir="ltr" translate="no">LossScale</code> instance associated with this optimizer.</li>
<li><b><code dir="ltr" translate="no">lr</code></b></li>
<li><b><code dir="ltr" translate="no">weights</code></b>:   Returns variables of this Optimizer based on the order created.</li>
</ul>
<h2 id="methods_2">Methods</h2>
<h3 id="add_slot"><code dir="ltr" translate="no">add_slot</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L329-L333" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">add_slot(
    var, slot_name, initializer=&#39;zeros&#39;
)
</code></pre>
<p>Add a new slot variable for <code dir="ltr" translate="no">var</code>.</p>
<h3 id="add_weight"><code dir="ltr" translate="no">add_weight</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L780-L820" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">add_weight(
    name, shape, dtype=None, initializer=&#39;zeros&#39;, trainable=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE
)
</code></pre>
<h3 id="apply_gradients"><code dir="ltr" translate="no">apply_gradients</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L219-L224" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">apply_gradients(
    grads_and_vars, name=None
)
</code></pre>
<p>Apply gradients to variables.</p>
<p>This is the second part of <code dir="ltr" translate="no">minimize()</code>. It returns an <code dir="ltr" translate="no">Operation</code> that
applies gradients.</p>
<h4 id="args_7">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">grads_and_vars</code></b>: List of (gradient, variable) pairs.</li>
<li><b><code dir="ltr" translate="no">name</code></b>: Optional name for the returned operation.  Default to the name
passed to the <code dir="ltr" translate="no">Optimizer</code> constructor.</li>
</ul>
<h4 id="returns_9">Returns:</h4>
<p>An <code dir="ltr" translate="no">Operation</code> that applies the specified gradients. The <code dir="ltr" translate="no">iterations</code>
will be automatically increased by 1.</p>
<h4 id="raises_4">Raises:</h4>
<ul>
<li><b><code dir="ltr" translate="no">TypeError</code></b>: If <code dir="ltr" translate="no">grads_and_vars</code> is malformed.</li>
<li><b><code dir="ltr" translate="no">ValueError</code></b>: If none of the variables have gradients.</li>
</ul>
<h3 id="from_config"><code dir="ltr" translate="no">from_config</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L261-L268" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">@classmethod
from_config(
    cls, config, custom_objects=None
)
</code></pre>
<p>Creates an optimizer from its config.</p>
<p>This method is the reverse of <code dir="ltr" translate="no">get_config</code>,
capable of instantiating the same optimizer from the config
dictionary.</p>
<h4 id="arguments_4">Arguments:</h4>
<ul>
<li><b><code dir="ltr" translate="no">config</code></b>: A Python dictionary, typically the output of get_config.</li>
<li><b><code dir="ltr" translate="no">custom_objects</code></b>: A Python dictionary mapping names to additional Python
objects used to create this optimizer, such as a function used for a
hyperparameter.</li>
</ul>
<h4 id="returns_10">Returns:</h4>
<p>An optimizer instance.</p>
<h3 id="get_config"><code dir="ltr" translate="no">get_config</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L253-L259" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">get_config()
</code></pre>
<p>Returns the config of the optimimizer.</p>
<p>An optimizer config is a Python dictionary (serializable)
containing the configuration of an optimizer.
The same optimizer can be reinstantiated later
(without any saved state) from this configuration.</p>
<h4 id="returns_11">Returns:</h4>
<p>Python dictionary.</p>
<h3 id="get_gradients"><code dir="ltr" translate="no">get_gradients</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L214-L217" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">get_gradients(
    loss, params
)
</code></pre>
<p>Returns gradients of <code dir="ltr" translate="no">loss</code> with respect to <code dir="ltr" translate="no">params</code>.</p>
<h4 id="arguments_5">Arguments:</h4>
<ul>
<li><b><code dir="ltr" translate="no">loss</code></b>: Loss tensor.</li>
<li><b><code dir="ltr" translate="no">params</code></b>: List of variables.</li>
</ul>
<h4 id="returns_12">Returns:</h4>
<p>List of gradient tensors.</p>
<h4 id="raises_5">Raises:</h4>
<ul>
<li><b><code dir="ltr" translate="no">ValueError</code></b>: In case any gradient cannot be computed (e.g. if gradient
function not implemented).</li>
</ul>
<h3 id="get_scaled_loss"><code dir="ltr" translate="no">get_scaled_loss</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L150-L177" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">get_scaled_loss(
    loss
)
</code></pre>
<p>Scales the loss by the loss scale.</p>
<p>This method is only needed if you compute gradients manually, e.g. with
<a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code dir="ltr" translate="no">tf.GradientTape</code></a>. In that case, call this method to scale the loss before
passing the loss to <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code dir="ltr" translate="no">tf.GradientTape</code></a>. If you use
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#minimize"><code dir="ltr" translate="no">LossScaleOptimizer.minimize</code></a> or <a href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer#get_gradients"><code dir="ltr" translate="no">LossScaleOptimizer.get_gradients</code></a>, loss
scaling is automatically applied and this method is unneeded.</p>
<p>If this method is called, <code dir="ltr" translate="no">get_unscaled_gradients</code> should also be called.
See the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer"><code dir="ltr" translate="no">tf.keras.mixed_precision.experimental.LossScaleOptimizer</code></a> doc for
an example.</p>
<h4 id="args_8">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">loss</code></b>: The loss, which will be multiplied by the loss scale. Can either be
a tensor or a callable returning a tensor.</li>
</ul>
<h4 id="returns_13">Returns:</h4>
<p><code dir="ltr" translate="no">loss</code> multiplied by <a href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer#loss_scale"><code dir="ltr" translate="no">LossScaleOptimizer.loss_scale()</code></a>.</p>
<h3 id="get_slot"><code dir="ltr" translate="no">get_slot</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L318-L327" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">get_slot(
    var, slot_name
)
</code></pre>
<h3 id="get_slot_names"><code dir="ltr" translate="no">get_slot_names</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L281-L282" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">get_slot_names()
</code></pre>
<p>A list of names for this optimizer&#39;s slots.</p>
<h3 id="get_unscaled_gradients"><code dir="ltr" translate="no">get_unscaled_gradients</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L179-L203" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">get_unscaled_gradients(
    grads
)
</code></pre>
<p>Unscales the gradients by the loss scale.</p>
<p>This method is only needed if you compute gradients manually, e.g. with
<a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code dir="ltr" translate="no">tf.GradientTape</code></a>. In that case, call this method to unscale the gradients
after computing them with <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code dir="ltr" translate="no">tf.GradientTape</code></a>. If you use
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#minimize"><code dir="ltr" translate="no">LossScaleOptimizer.minimize</code></a> or <a href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer#get_gradients"><code dir="ltr" translate="no">LossScaleOptimizer.get_gradients</code></a>, loss
scaling is automatically applied and this method is unneeded.</p>
<p>If this method is called, <code dir="ltr" translate="no">get_scaled_loss</code> should also be called. See
the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer"><code dir="ltr" translate="no">tf.keras.mixed_precision.experimental.LossScaleOptimizer</code></a> doc for an
example.</p>
<h4 id="args_9">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">grads</code></b>: A list of tensors, each which will be divided by the loss scale.
Can have None values, which are ignored.</li>
</ul>
<h4 id="returns_14">Returns:</h4>
<p>A new list the same size as <code dir="ltr" translate="no">grads</code>, where every non-None value in <code dir="ltr" translate="no">grads</code>
is divided by <a href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer#loss_scale"><code dir="ltr" translate="no">LossScaleOptimizer.loss_scale()</code></a>.</p>
<h3 id="get_updates"><code dir="ltr" translate="no">get_updates</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L502-L509" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">get_updates(
    loss, params
)
</code></pre>
<h3 id="get_weights"><code dir="ltr" translate="no">get_weights</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L291-L292" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">get_weights()
</code></pre>
<h3 id="minimize"><code dir="ltr" translate="no">minimize</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L289-L318" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">minimize(
    loss, var_list, grad_loss=None, name=None
)
</code></pre>
<p>Minimize <code dir="ltr" translate="no">loss</code> by updating <code dir="ltr" translate="no">var_list</code>.</p>
<p>This method simply computes gradient using <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code dir="ltr" translate="no">tf.GradientTape</code></a> and calls
<code dir="ltr" translate="no">apply_gradients()</code>. If you want to process the gradient before applying
then call <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code dir="ltr" translate="no">tf.GradientTape</code></a> and <code dir="ltr" translate="no">apply_gradients()</code> explicitly instead
of using this function.</p>
<h4 id="args_10">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">loss</code></b>: A callable taking no arguments which returns the value to minimize.</li>
<li><b><code dir="ltr" translate="no">var_list</code></b>: list or tuple of <code dir="ltr" translate="no">Variable</code> objects to update to minimize
<code dir="ltr" translate="no">loss</code>, or a callable returning the list or tuple of <code dir="ltr" translate="no">Variable</code> objects.
Use callable when the variable list would otherwise be incomplete before
<code dir="ltr" translate="no">minimize</code> since the variables are created at the first time <code dir="ltr" translate="no">loss</code> is
called.</li>
<li><b><code dir="ltr" translate="no">grad_loss</code></b>: Optional. A <code dir="ltr" translate="no">Tensor</code> holding the gradient computed for <code dir="ltr" translate="no">loss</code>.</li>
<li><b><code dir="ltr" translate="no">name</code></b>: Optional name for the returned operation.</li>
</ul>
<h4 id="returns_15">Returns:</h4>
<p>An <code dir="ltr" translate="no">Operation</code> that updates the variables in <code dir="ltr" translate="no">var_list</code>. The <code dir="ltr" translate="no">iterations</code>
will be automatically increased by 1.</p>
<h4 id="raises_6">Raises:</h4>
<ul>
<li><b><code dir="ltr" translate="no">ValueError</code></b>: If some of the variables are not <code dir="ltr" translate="no">Variable</code> objects.</li>
</ul>
<h3 id="set_weights"><code dir="ltr" translate="no">set_weights</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L294-L295" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">set_weights(
    weights
)
</code></pre>
<h3 id="variables"><code dir="ltr" translate="no">variables</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L284-L285" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">variables()
</code></pre>
<p>Returns variables of this Optimizer based on the order created.</p>
</div>
<devsite-page-rating hover-rating-star="0" position="footer" selected-rating="0">
</devsite-page-rating>
</article>
</article>

</devsite-content>
</main>
<devsite-footer-promos class="devsite-footer">
</devsite-footer-promos>
<devsite-footer-linkboxes class="devsite-footer">

</devsite-footer-linkboxes>
<devsite-footer-utility class="devsite-footer">
<div class="devsite-footer-utility nocontent">

</div>
</devsite-footer-utility>
</section></section>
<devsite-sitemask></devsite-sitemask>
<devsite-snackbar></devsite-snackbar> <devsite-tooltip></devsite-tooltip>
<devsite-heading-link></devsite-heading-link>
<devsite-analytics>


</devsite-analytics>
 
</body></html>