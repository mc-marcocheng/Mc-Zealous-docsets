<!DOCTYPE html><html dir="ltr" lang="en"><head>
<meta content="157101835696-ooapojlodmuabs2do2vuhhnf90bccmoi.apps.googleusercontent.com" name="google-signin-client-id"/>
<meta content="profile email" name="google-signin-scope"/>
<meta content="TensorFlow" property="og:site_name"/>
<meta content="website" property="og:type"/>
<meta content="#ff6f00" name="theme-color"/>
<meta charset="utf-8"/>
<meta content="IE=Edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<link crossorigin="use-credentials" href="_pwa/tensorflow/manifest.json" rel="manifest"/>
<link crossorigin="" href="/www.gstatic.com" rel="preconnect"/>
<link crossorigin="" href="/fonts.gstatic.com" rel="preconnect"/>
<link crossorigin="" href="/fonts.googleapis.com" rel="preconnect"/>
<link href="../../../../../main.css" rel="stylesheet"/>

<noscript>

</noscript>
<link href="https://www.gstatic.com/devrel-devsite/prod/v3e2dbdc40e7394635e5230ecc02cb28039ea55a5d72db9939d2fb9fc9e16d0ff/tensorflow/images/favicon.png" rel="shortcut icon"/>
<link href="https://www.gstatic.com/devrel-devsite/prod/v3e2dbdc40e7394635e5230ecc02cb28039ea55a5d72db9939d2fb9fc9e16d0ff/tensorflow/images/apple-touch-icon-180x180.png" rel="apple-touch-icon"/><link href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization" rel="canonical"/><link href="https://www.tensorflow.org/s/opensearch.xml" rel="search" title="TensorFlow" type="application/opensearchdescription+xml"/>
<title>tf.keras.layers.BatchNormalization &nbsp;|&nbsp; TensorFlow Core v2.1.0</title>
<meta content="tf.keras.layers.BatchNormalization &nbsp;|&nbsp; TensorFlow Core v2.1.0" property="og:title"/>
<meta content="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization" property="og:url"/>
<meta content="en" property="og:locale"/>

</head>
<body class="" layout="docs" pending="" theme="tensorflow-theme" type="reference">
<devsite-progress id="app-progress" type="indeterminate"></devsite-progress>
<section class="devsite-wrapper"> <devsite-book-nav scrollbars="">

</devsite-book-nav>
<section id="gc-wrapper">
<main class="devsite-main-content" has-book-nav="" has-toc="" role="main">
<devsite-toc class="devsite-nav"></devsite-toc>
<devsite-content>
<article class="devsite-article">
<article class="devsite-article-inner"><style>
        /* Styles inlined from /site-assets/css/style.css */
/* override theme */
table img {
  max-width: 100%;
}

/* override var element to differentiate color from comment */
var, var code, var span, .prettyprint var span {
  color: #039be5;
}

/* .devsite-terminal virtualenv prompt */
.tfo-terminal-venv::before {
  content: "(venv) $ " !important;
}

/* .devsite-terminal root prompt */
.tfo-terminal-root::before {
  content: "# " !important;
}

/* .devsite-terminal Windows prompt */
.tfo-terminal-windows::before {
  content: "C:\\> " !important;
}

/* .devsite-terminal Windows prompt w/ virtualenv */
.tfo-terminal-windows-venv::before {
  content: "(venv) C:\\> " !important;
}

.tfo-diff-green-one-level + * {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-green + * > * {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-green-list + ul > li:first-of-type {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-red-one-level + * {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

.tfo-diff-red + * > * {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

.tfo-diff-red-list + ul > li:first-of-type {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

devsite-code .tfo-notebook-code-cell-output {
  max-height: 300px;
  overflow: auto;
  background: rgba(255, 247, 237, 1);  /* orange bg to distinguish from input code cells */
}

devsite-code .tfo-notebook-code-cell-output + .devsite-code-buttons-container button {
  background: rgba(255, 247, 237, .7);  /* orange bg to distinguish from input code cells */
}

devsite-code[dark-code] .tfo-notebook-code-cell-output {
  background: rgba(64, 78, 103, 1);  /* medium slate */
}

devsite-code[dark-code] .tfo-notebook-code-cell-output + .devsite-code-buttons-container button {
  background: rgba(64, 78, 103, .7);  /* medium slate */
}

/* override default table styles for notebook buttons */
.devsite-table-wrapper .tfo-notebook-buttons {
  display: inline-block;
  margin-left: 3px;
  width: auto;
}

.tfo-notebook-buttons td {
  padding-left: 0;
  padding-right: 20px;
}

.tfo-notebook-buttons a,
.tfo-notebook-buttons :link,
.tfo-notebook-buttons :visited {
  border-radius: 8px;
  box-shadow: 0 1px 2px 0 rgba(60, 64, 67, .3), 0 1px 3px 1px rgba(60, 64, 67, .15);
  color: #202124;
  padding: 12px 24px;
  transition: box-shadow 0.2s;
}

.tfo-notebook-buttons a:hover,
.tfo-notebook-buttons a:focus {
  box-shadow: 0 1px 2px 0 rgba(60, 64, 67, .3), 0 2px 6px 2px rgba(60, 64, 67, .15);
}

.tfo-notebook-buttons tr {
  background: 0;
  border: 0;
}

/* on rendered notebook page,
   remove link to webpage since we're already here */
.tfo-notebook-buttons:not(.tfo-api) td:first-child {
  display: none;
}

.tfo-notebook-buttons td > a {
  -webkit-box-align: center;
  -ms-flex-align: center;
  align-items: center;
  display: -webkit-box;
  display: -ms-flexbox;
  display: flex;
}

.tfo-notebook-buttons td > a > img {
  margin-right: 8px;
}

/* landing pages */

.tfo-landing-row-item-inset-white {
  background-color: #fff;
  padding: 32px;
}

.tfo-landing-row-item-inset-white ol,
.tfo-landing-row-item-inset-white ul {
  padding-left: 20px;
}

/* colab callout button */
.colab-callout-row devsite-code {
  border-radius: 8px 8px 0 0;
  box-shadow: none;
}

.colab-callout-footer {
  background: #e3e4e7;
  border-radius: 0 0 8px 8px;
  color: #37474f;
  padding: 20px;
}

.colab-callout-row devsite-code[dark-code] + .colab-callout-footer {
  background: #3f4f66;
}


.colab-callout-footer > .button {
  margin-top: 4px;
  color: #ff5c00;
}

.colab-callout-footer > a > span {
  padding-top: 10px;
  vertical-align: middle;
  color: #37474f;
  padding-left: 10px;
  padding-right: 10px;
  font-size: 14px;
}

.colab-callout-row devsite-code[dark-code] + .colab-callout-footer > a > span {
  color: #fff;
}

a.colab-button {
  background: rgba(255, 255, 255, .75);
  border: solid 1px rgba(0, 0, 0, .08);
  border-bottom-color: rgba(0, 0, 0, .15);
  border-radius: 4px;
  color: #aaa;
  display: inline-block;
  font-size: 11px !important;
  font-weight: 300;
  line-height: 16px;
  padding: 4px 8px;
  text-decoration: none;
  text-transform: uppercase;
}

a.colab-button:hover {
  background: white;
  border-color: rgba(0, 0, 0, .2);
  color: #666;
}

a.colab-button span {
  background: url(/images/colab_logo_button.svg) no-repeat 1px 1px / 20px;
  border-radius: 4px;
  display: inline-block;
  padding-left: 24px;
  text-decoration: none;
}

@media screen and (max-width: 600px) {
  .tfo-notebook-buttons td {
    display: block;
  }
}

/* guide and tutorials landing page cards and sections */

.tfo-landing-page-card {
  padding: 16px;
  box-shadow: 0 0 36px rgba(0,0,0,0.1);
  border-radius: 10px;
}

/* Page section headings */
.tfo-landing-page-heading h2, h2.tfo-landing-page-heading {
  font-family: "Google Sans", sans-serif;
  color: #425066;
  font-size: 30px;
  font-weight: 700;
  line-height: 40px;
}

/* Item title headings */
.tfo-landing-page-heading h3, h3.tfo-landing-page-heading,
.tfo-landing-page-card h3, h3.tfo-landing-page-card {
  font-family: "Google Sans", sans-serif;
  color: #425066;
  font-size: 20px;
  font-weight: 500;
  line-height: 26px;
}

/* List of tutorials notebooks for subsites */
.tfo-landing-page-resources-ul {
  padding-left: 15px
}

.tfo-landing-page-resources-ul > li {
  margin: 6px 0;
}

/* Temporary fix to hide product description in header on landing pages */
devsite-header .devsite-product-description {
  display: none;
}

        </style> <div class="devsite-banner devsite-banner-announcement">
<div class="devsite-banner-message">
<div class="devsite-banner-message-text">
            Missed TensorFlow Dev Summit? Check out the video playlist. <a class="button button-primary button-tfo-announcement" href="https://goo.gle/TFDS20AllSessions">Watch recordings</a>
</div>
</div>
</div>
<div class="devsite-article-meta">
<ul class="devsite-breadcrumb-list">
<li class="devsite-breadcrumb-item">
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="1" href="">
            TensorFlow
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="2" href="api">
            API
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="3" href="api_docs">
            TensorFlow Core v2.1.0
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="4" href="api_docs/python/tf">
            Python
      
  </a>
</li>
</ul>
<devsite-page-rating hover-rating-star="0" position="header" selected-rating="0">
</devsite-page-rating>
</div>
<a class="dashingAutolink" name="autolink-1861"></a><a class="dashAnchor" name="//apple_ref/cpp/Function/tf.keras.layers.BatchNormalization"></a><h1 class="dash-function">tf.keras.layers.BatchNormalization</h1>
<devsite-toc class="devsite-nav" devsite-toc-embedded="">
</devsite-toc>
<div class="devsite-article-body clearfix">
<p><devsite-mathjax config="TeX-AMS-MML_SVG"></devsite-mathjax>
</p>
<!-- DO NOT EDIT! Automatically generated file. -->
<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<meta content="tf.keras.layers.BatchNormalization" itemprop="name"/>
<meta content="Stable" itemprop="path"/>
<meta content="__init__" itemprop="property"/>
</div>
<p><devsite-nav-buttons name="version" param="reset">
<button default="" value="stable">See Stable</button>
<button value="nightly">See Nightly</button>
</devsite-nav-buttons></p>
<!-- Stable -->
<table align="left" class="tfo-notebook-buttons tfo-api">
<tbody><tr><td>
<a href="versions/r1.15/api_docs/python/tf/keras/layers/BatchNormalization" target="_blank">
<img src="https://www.tensorflow.org/images/tf_logo_32px.png"/>
  TensorFlow 1 version</a>
</td>
<td>
<a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/layers/normalization_v2.py#L26-L65" target="_blank">
<img src="https://www.tensorflow.org/images/GitHub-Mark-32px.png"/>
    View source on GitHub
  </a>
</td></tr></tbody></table>
<p>Normalize and scale inputs or activations. (Ioffe and Szegedy, 2014).</p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">tf.keras.layers.BatchNormalization(
    axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
    beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
    moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
    beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
    gamma_constraint=None, renorm=False, renorm_clipping=None, renorm_momentum=0.99,
    fused=None, trainable=True, virtual_batch_size=None, adjustment=None, name=None,
    **kwargs
)
</code></pre>
<h3>Used in the notebooks</h3>
<table class="vertical-rules">
<thead>
<tr>
<th>Used in the guide</th>
<th>Used in the tutorials</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<ul>
<li><a href="https://www.tensorflow.org/guide/migrate">Migrate your TensorFlow 1 code to TensorFlow 2</a></li>
<li><a href="https://www.tensorflow.org/guide/keras/rnn">Recurrent Neural Networks (RNN) with Keras</a></li>
</ul>
</td>
<td>
<ul>
<li><a href="https://www.tensorflow.org/tutorials/customization/custom_layers">Custom layers</a></li>
<li><a href="https://www.tensorflow.org/tutorials/generative/pix2pix">Pix2Pix</a></li>
<li><a href="https://www.tensorflow.org/tutorials/generative/dcgan">Deep Convolutional Generative Adversarial Network</a></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Normalize the activations of the previous layer at each batch,
i.e. applies a transformation that maintains the mean activation
close to 0 and the activation standard deviation close to 1.</p>
<p>Batch normalization differs from other layers in several key aspects:</p>
<p>1) Adding BatchNormalization with <code dir="ltr" translate="no">training=True</code> to a model causes the
result of one example to depend on the contents of all other examples in a
minibatch. Be careful when padding batches or masking examples, as these can
change the minibatch statistics and affect other examples.</p>
<p>2) Updates to the weights (moving statistics) are based on the forward pass
of a model rather than the result of gradient computations.</p>
<p>3) When performing inference using a model containing batch normalization, it
is generally (though not always) desirable to use accumulated statistics
rather than mini-batch statistics. This is acomplished by passing
<code dir="ltr" translate="no">training=False</code> when calling the model, or using <code dir="ltr" translate="no">model.predict</code>.</p>
<h4 id="arguments_2">Arguments:</h4>
<ul>
<li><b><code dir="ltr" translate="no">axis</code></b>: Integer, the axis that should be normalized
(typically the features axis).
For instance, after a <code dir="ltr" translate="no">Conv2D</code> layer with
<code dir="ltr" translate="no">data_format=&#34;channels_first&#34;</code>,
set <code dir="ltr" translate="no">axis=1</code> in <code dir="ltr" translate="no">BatchNormalization</code>.</li>
<li><b><code dir="ltr" translate="no">momentum</code></b>: Momentum for the moving average.</li>
<li><b><code dir="ltr" translate="no">epsilon</code></b>: Small float added to variance to avoid dividing by zero.</li>
<li><b><code dir="ltr" translate="no">center</code></b>: If True, add offset of <code dir="ltr" translate="no">beta</code> to normalized tensor.
If False, <code dir="ltr" translate="no">beta</code> is ignored.</li>
<li><b><code dir="ltr" translate="no">scale</code></b>: If True, multiply by <code dir="ltr" translate="no">gamma</code>.
If False, <code dir="ltr" translate="no">gamma</code> is not used.
When the next layer is linear (also e.g. <a href="https://www.tensorflow.org/api_docs/python/tf/nn/relu"><code dir="ltr" translate="no">nn.relu</code></a>),
this can be disabled since the scaling
will be done by the next layer.</li>
<li><b><code dir="ltr" translate="no">beta_initializer</code></b>: Initializer for the beta weight.</li>
<li><b><code dir="ltr" translate="no">gamma_initializer</code></b>: Initializer for the gamma weight.</li>
<li><b><code dir="ltr" translate="no">moving_mean_initializer</code></b>: Initializer for the moving mean.</li>
<li><b><code dir="ltr" translate="no">moving_variance_initializer</code></b>: Initializer for the moving variance.</li>
<li><b><code dir="ltr" translate="no">beta_regularizer</code></b>: Optional regularizer for the beta weight.</li>
<li><b><code dir="ltr" translate="no">gamma_regularizer</code></b>: Optional regularizer for the gamma weight.</li>
<li><b><code dir="ltr" translate="no">beta_constraint</code></b>: Optional constraint for the beta weight.</li>
<li><b><code dir="ltr" translate="no">gamma_constraint</code></b>: Optional constraint for the gamma weight.</li>
<li><b><code dir="ltr" translate="no">renorm</code></b>: Whether to use Batch Renormalization
(https://arxiv.org/abs/1702.03275). This adds extra variables during
training. The inference is the same for either value of this parameter.</li>
<li><b><code dir="ltr" translate="no">renorm_clipping</code></b>: A dictionary that may map keys &#39;rmax&#39;, &#39;rmin&#39;, &#39;dmax&#39; to
scalar <code dir="ltr" translate="no">Tensors</code> used to clip the renorm correction. The correction
<code dir="ltr" translate="no">(r, d)</code> is used as <code dir="ltr" translate="no">corrected_value = normalized_value * r + d</code>, with
<code dir="ltr" translate="no">r</code> clipped to [rmin, rmax], and <code dir="ltr" translate="no">d</code> to [-dmax, dmax]. Missing rmax, rmin,
dmax are set to inf, 0, inf, respectively.</li>
<li><b><code dir="ltr" translate="no">renorm_momentum</code></b>: Momentum used to update the moving means and standard
deviations with renorm. Unlike <code dir="ltr" translate="no">momentum</code>, this affects training
and should be neither too small (which would add noise) nor too large
(which would give stale estimates). Note that <code dir="ltr" translate="no">momentum</code> is still applied
to get the means and variances for inference.</li>
<li><b><code dir="ltr" translate="no">fused</code></b>: if <code dir="ltr" translate="no">True</code>, use a faster, fused implementation, or raise a ValueError
if the fused implementation cannot be used. If <code dir="ltr" translate="no">None</code>, use the faster
implementation if possible. If False, do not used the fused
implementation.</li>
<li><b><code dir="ltr" translate="no">trainable</code></b>: Boolean, if <code dir="ltr" translate="no">True</code> the variables will be marked as trainable.</li>
<li><b><code dir="ltr" translate="no">virtual_batch_size</code></b>: An <code dir="ltr" translate="no">int</code>. By default, <code dir="ltr" translate="no">virtual_batch_size</code> is <code dir="ltr" translate="no">None</code>,
which means batch normalization is performed across the whole batch. When
<code dir="ltr" translate="no">virtual_batch_size</code> is not <code dir="ltr" translate="no">None</code>, instead perform &#34;Ghost Batch
Normalization&#34;, which creates virtual sub-batches which are each
normalized separately (with shared gamma, beta, and moving statistics).
Must divide the actual batch size during execution.</li>
<li><b><code dir="ltr" translate="no">adjustment</code></b>: A function taking the <code dir="ltr" translate="no">Tensor</code> containing the (dynamic) shape of
the input tensor and returning a pair (scale, bias) to apply to the
normalized values (before gamma and beta), only during training. For
example, if axis==-1,
<code dir="ltr" translate="no">adjustment = lambda shape: (
  tf.random.uniform(shape[-1:], 0.93, 1.07),
  tf.random.uniform(shape[-1:], -0.1, 0.1))</code>
will scale the normalized value by up to 7% up or down, then shift the
result by up to 0.1 (with independent scaling and bias for each feature
but shared across all examples), and finally apply gamma and/or beta. If
<code dir="ltr" translate="no">None</code>, no adjustment is applied. Cannot be specified if
virtual_batch_size is specified.</li>
</ul>
<h4 id="call_arguments_2">Call arguments:</h4>
<ul>
<li><b><code dir="ltr" translate="no">inputs</code></b>: Input tensor (of any rank).</li>
<li><b><code dir="ltr" translate="no">training</code></b>: Python boolean indicating whether the layer should behave in
training mode or in inference mode.
<ul>
<li><code dir="ltr" translate="no">training=True</code>: The layer will normalize its inputs using the
mean and variance of the current batch of inputs.</li>
<li><code dir="ltr" translate="no">training=False</code>: The layer will normalize its inputs using the
mean and variance of its moving statistics, learned during training.</li>
</ul></li>
</ul>
<h4 id="input_shape_2">Input shape:</h4>
<p>Arbitrary. Use the keyword argument <code dir="ltr" translate="no">input_shape</code>
(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<h4 id="output_shape_2">Output shape:</h4>
<p>Same shape as input.</p>
<p><strong>About setting <code dir="ltr" translate="no">layer.trainable = False</code> on a `BatchNormalization layer:</strong></p>
<p>The meaning of setting <code dir="ltr" translate="no">layer.trainable = False</code> is to freeze the layer,
i.e. its internal state will not change during training:
its trainable weights will not be updated
during <code dir="ltr" translate="no">fit()</code> or <code dir="ltr" translate="no">train_on_batch()</code>, and its state updates will not be run.</p>
<p>Usually, this does not necessarily mean that the layer is run in inference
mode (which is normally controlled by the <code dir="ltr" translate="no">training</code> argument that can
be passed when calling a layer). &#34;Frozen state&#34; and &#34;inference mode&#34;
are two separate concepts.</p>
<p>However, in the case of the <code dir="ltr" translate="no">BatchNormalization</code> layer, <strong>setting
<code dir="ltr" translate="no">trainable = False</code> on the layer means that the layer will be
subsequently run in inference mode</strong> (meaning that it will use
the moving mean and the moving variance to normalize the current batch,
rather than using the mean and variance of the current batch).</p>
<p>This behavior has been introduced in TensorFlow 2.0, in order
to enable <code dir="ltr" translate="no">layer.trainable = False</code> to produce the most commonly
expected behavior in the convnet fine-tuning use case.</p>
<h4 id="note_that_2">Note that:</h4>
<ul>
<li>This behavior only occurs as of TensorFlow 2.0. In 1.*,
setting <code dir="ltr" translate="no">layer.trainable = False</code> would freeze the layer but would
not switch it to inference mode.</li>
<li>Setting <code dir="ltr" translate="no">trainable</code> on an model containing other layers will
recursively set the <code dir="ltr" translate="no">trainable</code> value of all inner layers.</li>
<li>If the value of the <code dir="ltr" translate="no">trainable</code>
attribute is changed after calling <code dir="ltr" translate="no">compile()</code> on a model,
the new value doesn&#39;t take effect for this model
until <code dir="ltr" translate="no">compile()</code> is called again.</li>
</ul>
<p>Normalization equations:
  Consider the intermediate activations (x) of a mini-batch of size
  (m):</p>
<p>We can compute the mean and variance of the batch</p>
<p>({\mu<em>B} = \frac{1}{m} \sum</em>{i=1}^{m} {x_i})</p>
<p>({\sigma<em>B^2} = \frac{1}{m} \sum</em>{i=1}^{m} ({x_i} - {\mu_B})^2)</p>
<p>and then compute a normalized (x), including a small factor
  ({\epsilon}) for numerical stability.</p>
<p>(\hat{x_i} = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}})</p>
<p>And finally (\hat{x}) is linearly transformed by ({\gamma})
  and ({\beta}), which are learned parameters:</p>
<p>({y_i} = {\gamma * \hat{x_i} + \beta})</p>
<h4 id="references_2">References:</h4>
<ul>
<li><a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a></li>
</ul>
</div>
<devsite-page-rating hover-rating-star="0" position="footer" selected-rating="0">
</devsite-page-rating>
</article>
</article>

</devsite-content>
</main>
<devsite-footer-promos class="devsite-footer">
</devsite-footer-promos>
<devsite-footer-linkboxes class="devsite-footer">

</devsite-footer-linkboxes>
<devsite-footer-utility class="devsite-footer">
<div class="devsite-footer-utility nocontent">

</div>
</devsite-footer-utility>
</section></section>
<devsite-sitemask></devsite-sitemask>
<devsite-snackbar></devsite-snackbar> <devsite-tooltip></devsite-tooltip>
<devsite-heading-link></devsite-heading-link>
<devsite-analytics>


</devsite-analytics>
 
</body></html>