<!DOCTYPE html><html dir="ltr" lang="en"><head>
<meta content="157101835696-ooapojlodmuabs2do2vuhhnf90bccmoi.apps.googleusercontent.com" name="google-signin-client-id"/>
<meta content="profile email" name="google-signin-scope"/>
<meta content="TensorFlow" property="og:site_name"/>
<meta content="website" property="og:type"/>
<meta content="#ff6f00" name="theme-color"/>
<meta charset="utf-8"/>
<meta content="IE=Edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<link crossorigin="use-credentials" href="_pwa/tensorflow/manifest.json" rel="manifest"/>
<link crossorigin="" href="/www.gstatic.com" rel="preconnect"/>
<link crossorigin="" href="/fonts.gstatic.com" rel="preconnect"/>
<link crossorigin="" href="/fonts.googleapis.com" rel="preconnect"/>
<link href="../../../main.css" rel="stylesheet"/>

<noscript>

</noscript>
<link href="https://www.gstatic.com/devrel-devsite/prod/v3e2dbdc40e7394635e5230ecc02cb28039ea55a5d72db9939d2fb9fc9e16d0ff/tensorflow/images/favicon.png" rel="shortcut icon"/>
<link href="https://www.gstatic.com/devrel-devsite/prod/v3e2dbdc40e7394635e5230ecc02cb28039ea55a5d72db9939d2fb9fc9e16d0ff/tensorflow/images/apple-touch-icon-180x180.png" rel="apple-touch-icon"/><link href="https://www.tensorflow.org/api_docs/python/tf/distribute" rel="canonical"/><link href="https://www.tensorflow.org/s/opensearch.xml" rel="search" title="TensorFlow" type="application/opensearchdescription+xml"/>
<title>Module: tf.distribute &nbsp;|&nbsp; TensorFlow Core v2.1.0</title>
<meta content="Module: tf.distribute &nbsp;|&nbsp; TensorFlow Core v2.1.0" property="og:title"/>
<meta content="https://www.tensorflow.org/api_docs/python/tf/distribute" property="og:url"/>
<meta content="en" property="og:locale"/>

</head>
<body class="" layout="docs" pending="" theme="tensorflow-theme" type="reference">
<devsite-progress id="app-progress" type="indeterminate"></devsite-progress>
<section class="devsite-wrapper"> <devsite-book-nav scrollbars="">

</devsite-book-nav>
<section id="gc-wrapper">
<main class="devsite-main-content" has-book-nav="" has-toc="" role="main">
<devsite-toc class="devsite-nav"></devsite-toc>
<devsite-content>
<article class="devsite-article">
<article class="devsite-article-inner"><style>
        /* Styles inlined from /site-assets/css/style.css */
/* override theme */
table img {
  max-width: 100%;
}

/* override var element to differentiate color from comment */
var, var code, var span, .prettyprint var span {
  color: #039be5;
}

/* .devsite-terminal virtualenv prompt */
.tfo-terminal-venv::before {
  content: "(venv) $ " !important;
}

/* .devsite-terminal root prompt */
.tfo-terminal-root::before {
  content: "# " !important;
}

/* .devsite-terminal Windows prompt */
.tfo-terminal-windows::before {
  content: "C:\\> " !important;
}

/* .devsite-terminal Windows prompt w/ virtualenv */
.tfo-terminal-windows-venv::before {
  content: "(venv) C:\\> " !important;
}

.tfo-diff-green-one-level + * {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-green + * > * {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-green-list + ul > li:first-of-type {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-red-one-level + * {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

.tfo-diff-red + * > * {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

.tfo-diff-red-list + ul > li:first-of-type {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

devsite-code .tfo-notebook-code-cell-output {
  max-height: 300px;
  overflow: auto;
  background: rgba(255, 247, 237, 1);  /* orange bg to distinguish from input code cells */
}

devsite-code .tfo-notebook-code-cell-output + .devsite-code-buttons-container button {
  background: rgba(255, 247, 237, .7);  /* orange bg to distinguish from input code cells */
}

devsite-code[dark-code] .tfo-notebook-code-cell-output {
  background: rgba(64, 78, 103, 1);  /* medium slate */
}

devsite-code[dark-code] .tfo-notebook-code-cell-output + .devsite-code-buttons-container button {
  background: rgba(64, 78, 103, .7);  /* medium slate */
}

/* override default table styles for notebook buttons */
.devsite-table-wrapper .tfo-notebook-buttons {
  display: inline-block;
  margin-left: 3px;
  width: auto;
}

.tfo-notebook-buttons td {
  padding-left: 0;
  padding-right: 20px;
}

.tfo-notebook-buttons a,
.tfo-notebook-buttons :link,
.tfo-notebook-buttons :visited {
  border-radius: 8px;
  box-shadow: 0 1px 2px 0 rgba(60, 64, 67, .3), 0 1px 3px 1px rgba(60, 64, 67, .15);
  color: #202124;
  padding: 12px 24px;
  transition: box-shadow 0.2s;
}

.tfo-notebook-buttons a:hover,
.tfo-notebook-buttons a:focus {
  box-shadow: 0 1px 2px 0 rgba(60, 64, 67, .3), 0 2px 6px 2px rgba(60, 64, 67, .15);
}

.tfo-notebook-buttons tr {
  background: 0;
  border: 0;
}

/* on rendered notebook page,
   remove link to webpage since we're already here */
.tfo-notebook-buttons:not(.tfo-api) td:first-child {
  display: none;
}

.tfo-notebook-buttons td > a {
  -webkit-box-align: center;
  -ms-flex-align: center;
  align-items: center;
  display: -webkit-box;
  display: -ms-flexbox;
  display: flex;
}

.tfo-notebook-buttons td > a > img {
  margin-right: 8px;
}

/* landing pages */

.tfo-landing-row-item-inset-white {
  background-color: #fff;
  padding: 32px;
}

.tfo-landing-row-item-inset-white ol,
.tfo-landing-row-item-inset-white ul {
  padding-left: 20px;
}

/* colab callout button */
.colab-callout-row devsite-code {
  border-radius: 8px 8px 0 0;
  box-shadow: none;
}

.colab-callout-footer {
  background: #e3e4e7;
  border-radius: 0 0 8px 8px;
  color: #37474f;
  padding: 20px;
}

.colab-callout-row devsite-code[dark-code] + .colab-callout-footer {
  background: #3f4f66;
}


.colab-callout-footer > .button {
  margin-top: 4px;
  color: #ff5c00;
}

.colab-callout-footer > a > span {
  padding-top: 10px;
  vertical-align: middle;
  color: #37474f;
  padding-left: 10px;
  padding-right: 10px;
  font-size: 14px;
}

.colab-callout-row devsite-code[dark-code] + .colab-callout-footer > a > span {
  color: #fff;
}

a.colab-button {
  background: rgba(255, 255, 255, .75);
  border: solid 1px rgba(0, 0, 0, .08);
  border-bottom-color: rgba(0, 0, 0, .15);
  border-radius: 4px;
  color: #aaa;
  display: inline-block;
  font-size: 11px !important;
  font-weight: 300;
  line-height: 16px;
  padding: 4px 8px;
  text-decoration: none;
  text-transform: uppercase;
}

a.colab-button:hover {
  background: white;
  border-color: rgba(0, 0, 0, .2);
  color: #666;
}

a.colab-button span {
  background: url(/images/colab_logo_button.svg) no-repeat 1px 1px / 20px;
  border-radius: 4px;
  display: inline-block;
  padding-left: 24px;
  text-decoration: none;
}

@media screen and (max-width: 600px) {
  .tfo-notebook-buttons td {
    display: block;
  }
}

/* guide and tutorials landing page cards and sections */

.tfo-landing-page-card {
  padding: 16px;
  box-shadow: 0 0 36px rgba(0,0,0,0.1);
  border-radius: 10px;
}

/* Page section headings */
.tfo-landing-page-heading h2, h2.tfo-landing-page-heading {
  font-family: "Google Sans", sans-serif;
  color: #425066;
  font-size: 30px;
  font-weight: 700;
  line-height: 40px;
}

/* Item title headings */
.tfo-landing-page-heading h3, h3.tfo-landing-page-heading,
.tfo-landing-page-card h3, h3.tfo-landing-page-card {
  font-family: "Google Sans", sans-serif;
  color: #425066;
  font-size: 20px;
  font-weight: 500;
  line-height: 26px;
}

/* List of tutorials notebooks for subsites */
.tfo-landing-page-resources-ul {
  padding-left: 15px
}

.tfo-landing-page-resources-ul > li {
  margin: 6px 0;
}

/* Temporary fix to hide product description in header on landing pages */
devsite-header .devsite-product-description {
  display: none;
}

        </style> <div class="devsite-banner devsite-banner-announcement">
<div class="devsite-banner-message">
<div class="devsite-banner-message-text">
            Missed TensorFlow Dev Summit? Check out the video playlist. <a class="button button-primary button-tfo-announcement" href="https://goo.gle/TFDS20AllSessions">Watch recordings</a>
</div>
</div>
</div>
<div class="devsite-article-meta">
<ul class="devsite-breadcrumb-list">
<li class="devsite-breadcrumb-item">
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="1" href="">
            TensorFlow
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="2" href="api">
            API
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="3" href="api_docs">
            TensorFlow Core v2.1.0
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="4" href="api_docs/python/tf">
            Python
      
  </a>
</li>
</ul>
<devsite-page-rating hover-rating-star="0" position="header" selected-rating="0">
</devsite-page-rating>
</div>
<a class="dashingAutolink" name="autolink-1292"></a><a class="dashAnchor" name="//apple_ref/cpp/Function/Module%3A%20tf.distribute"></a><h1 class="dash-function">Module: tf.distribute</h1>
<devsite-toc class="devsite-nav" devsite-toc-embedded="">
</devsite-toc>
<div class="devsite-article-body clearfix">
<p></p>
<!-- DO NOT EDIT! Automatically generated file. -->
<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<meta content="tf.distribute" itemprop="name"/>
<meta content="Stable" itemprop="path"/>
</div>
<p><devsite-nav-buttons name="version" param="reset">
<button default="" value="stable">See Stable</button>
<button value="nightly">See Nightly</button>
</devsite-nav-buttons></p>
<!-- Stable -->
<table align="left" class="tfo-notebook-buttons tfo-api">
<tbody><tr><td>
<a href="versions/r1.15/api_docs/python/tf/distribute" target="_blank">
<img src="https://www.tensorflow.org/images/tf_logo_32px.png"/>
  TensorFlow 1 version</a>
</td>
</tr></tbody></table>
<p>Library for running a computation across multiple devices.</p>
<p>See the guide for overview and examples:
<a href="https://www.tensorflow.org/guide/distributed_training">TensorFlow v2.x</a>,
<a href="https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/distribute_strategy.ipynb">TensorFlow v1.x</a>.  # pylint: disable=line-too-long</p>
<p>The intent of this library is that you can write an algorithm in a stylized way
and it will be usable with a variety of different <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"><code dir="ltr" translate="no">tf.distribute.Strategy</code></a>
implementations. Each descendant will implement a different strategy for
distributing the algorithm across multiple devices/machines.  Furthermore, these
changes can be hidden inside the specific layers and other library classes that
need special treatment to run in a distributed setting, so that most users&#39;
model definition code can run unchanged. The <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"><code dir="ltr" translate="no">tf.distribute.Strategy</code></a> API works
the same way with eager and graph execution.</p>
<p><em>Glossary</em></p>
<ul>
<li><em>Data parallelism</em> is where we run multiple copies of the model
on different slices of the input data. This is in contrast to
<em>model parallelism</em> where we divide up a single copy of a model
across multiple devices.
Note: we only support data parallelism for now, but
hope to add support for model parallelism in the future.</li>
<li>A <em>device</em> is a CPU or accelerator (e.g. GPUs, TPUs) on some machine that
TensorFlow can run operations on (see e.g. <a href="https://www.tensorflow.org/api_docs/python/tf/device"><code dir="ltr" translate="no">tf.device</code></a>). You may have multiple
devices on a single machine, or be connected to devices on multiple
machines. Devices used to run computations are called <em>worker devices</em>.
Devices used to store variables are <em>parameter devices</em>. For some strategies,
such as <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy"><code dir="ltr" translate="no">tf.distribute.MirroredStrategy</code></a>, the worker and parameter devices
will be the same (see mirrored variables below). For others they will be
different.  For example, <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CentralStorageStrategy"><code dir="ltr" translate="no">tf.distribute.experimental.CentralStorageStrategy</code></a>
puts the variables on a single device (which may be a worker device or may be
the CPU), and <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy"><code dir="ltr" translate="no">tf.distribute.experimental.ParameterServerStrategy</code></a> puts the
variables on separate machines called parameter servers (see below).</li>
<li>A <em>replica</em> is one copy of the model, running on one slice of the
input data. Right now each replica is executed on its own
worker device, but once we add support for model parallelism
a replica may span multiple worker devices.</li>
<li>A <em>host</em> is the CPU device on a machine with worker devices, typically
used for running input pipelines.</li>
<li>A <em>worker</em> is defined to be the physical machine(s) containing the physical
devices (e.g. GPUs, TPUs) on which the replicated computation is executed. A
worker may contain one or more replicas, but contains at least one
replica. Typically one worker will correspond to one machine, but in the case
of very large models with model parallelism, one worker may span multiple
machines. We typically run one input pipeline per worker, feeding all the
replicas on that worker.</li>
<li><em>Synchronous</em>, or more commonly <em>sync</em>, training is where the updates from
each replica are aggregated together before updating the model variables. This
is in contrast to <em>asynchronous</em>, or <em>async</em> training, where each replica
updates the model variables independently. You may also have replicas
partitioned into groups which are in sync within each group but async between
groups.</li>
<li><em>Parameter servers</em>: These are machines that hold a single copy of
parameters/variables, used by some strategies (right now just
<a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy"><code dir="ltr" translate="no">tf.distribute.experimental.ParameterServerStrategy</code></a>). All replicas that want
to operate on a variable retrieve it at the beginning of a step and send an
update to be applied at the end of the step. These can in priniciple support
either sync or async training, but right now we only have support for async
training with parameter servers. Compare to
<a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CentralStorageStrategy"><code dir="ltr" translate="no">tf.distribute.experimental.CentralStorageStrategy</code></a>, which puts all variables
on a single device on the same machine (and does sync training), and
<a href="https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy"><code dir="ltr" translate="no">tf.distribute.MirroredStrategy</code></a>, which mirrors variables to multiple devices
(see below).</li>
<li><em>Mirrored variables</em>: These are variables that are copied to multiple
devices, where we keep the copies in sync by applying the same
updates to every copy. Normally would only be used with sync training.</li>
<li>Reductions and all-reduce: A <em>reduction</em> is some method of aggregating
multiple values into one value, like &#34;sum&#34; or &#34;mean&#34;. If a strategy is doing
sync training, we will perform a reduction on the gradients to a parameter
from all replicas before applying the update. <em>All-reduce</em> is an algorithm for
performing a reduction on values from multiple devices and making the result
available on all of those devices.</li>
</ul>
<p>Note that we provide a default version of <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"><code dir="ltr" translate="no">tf.distribute.Strategy</code></a> that is
used when no other strategy is in scope, that provides the same API with
reasonable default behavior.</p>
<h2 id="modules_2">Modules</h2>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver"><code dir="ltr" translate="no">cluster_resolver</code></a> module: Library imports for ClusterResolvers.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental"><code dir="ltr" translate="no">experimental</code></a> module: Experimental Distribution Strategy library.</p>
<h2 id="classes_2">Classes</h2>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps"><code dir="ltr" translate="no">class CrossDeviceOps</code></a>: Base class for cross-device reduction and broadcasting algorithms.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/HierarchicalCopyAllReduce"><code dir="ltr" translate="no">class HierarchicalCopyAllReduce</code></a>: Reduction using hierarchical copy all-reduce.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext"><code dir="ltr" translate="no">class InputContext</code></a>: A class wrapping information needed by an input function.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/InputReplicationMode"><code dir="ltr" translate="no">class InputReplicationMode</code></a>: Replication mode for input function.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy"><code dir="ltr" translate="no">class MirroredStrategy</code></a>: Mirrors vars to distribute across multiple devices and machines.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/NcclAllReduce"><code dir="ltr" translate="no">class NcclAllReduce</code></a>: Reduction using NCCL all-reduce.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy"><code dir="ltr" translate="no">class OneDeviceStrategy</code></a>: A distribution strategy for running on a single device.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp"><code dir="ltr" translate="no">class ReduceOp</code></a>: Indicates how a set of values should be reduced.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice"><code dir="ltr" translate="no">class ReductionToOneDevice</code></a>: Always do reduction to one device first and then do broadcasting.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext"><code dir="ltr" translate="no">class ReplicaContext</code></a>: <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"><code dir="ltr" translate="no">tf.distribute.Strategy</code></a> API when in a replica context.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Server"><code dir="ltr" translate="no">class Server</code></a>: An in-process TensorFlow server, for use in distributed training.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"><code dir="ltr" translate="no">class Strategy</code></a>: A state &amp; compute distribution policy on a list of devices.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended"><code dir="ltr" translate="no">class StrategyExtended</code></a>: Additional APIs for algorithms that need to be distribution-aware.</p>
<h2 id="functions_2">Functions</h2>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental_set_strategy"><code dir="ltr" translate="no">experimental_set_strategy(...)</code></a>: Set a <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"><code dir="ltr" translate="no">tf.distribute.Strategy</code></a> as current without <code dir="ltr" translate="no">with strategy.scope()</code>.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context"><code dir="ltr" translate="no">get_replica_context(...)</code></a>: Returns the current <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext"><code dir="ltr" translate="no">tf.distribute.ReplicaContext</code></a> or <code dir="ltr" translate="no">None</code>.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy"><code dir="ltr" translate="no">get_strategy(...)</code></a>: Returns the current <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"><code dir="ltr" translate="no">tf.distribute.Strategy</code></a> object.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/has_strategy"><code dir="ltr" translate="no">has_strategy(...)</code></a>: Return if there is a current non-default <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"><code dir="ltr" translate="no">tf.distribute.Strategy</code></a>.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/in_cross_replica_context"><code dir="ltr" translate="no">in_cross_replica_context(...)</code></a>: Returns <code dir="ltr" translate="no">True</code> if in a cross-replica context.</p>
</div>
<devsite-page-rating hover-rating-star="0" position="footer" selected-rating="0">
</devsite-page-rating>
</article>
</article>

</devsite-content>
</main>
<devsite-footer-promos class="devsite-footer">
</devsite-footer-promos>
<devsite-footer-linkboxes class="devsite-footer">

</devsite-footer-linkboxes>
<devsite-footer-utility class="devsite-footer">
<div class="devsite-footer-utility nocontent">

</div>
</devsite-footer-utility>
</section></section>
<devsite-sitemask></devsite-sitemask>
<devsite-snackbar></devsite-snackbar> <devsite-tooltip></devsite-tooltip>
<devsite-heading-link></devsite-heading-link>
<devsite-analytics>


</devsite-analytics>
 
</body></html>