<!DOCTYPE html><html dir="ltr" lang="en"><head>
<meta content="157101835696-ooapojlodmuabs2do2vuhhnf90bccmoi.apps.googleusercontent.com" name="google-signin-client-id"/>
<meta content="profile email" name="google-signin-scope"/>
<meta content="TensorFlow" property="og:site_name"/>
<meta content="website" property="og:type"/>
<meta content="#ff6f00" name="theme-color"/>
<meta charset="utf-8"/>
<meta content="IE=Edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<link crossorigin="use-credentials" href="_pwa/tensorflow/manifest.json" rel="manifest"/>
<link crossorigin="" href="/www.gstatic.com" rel="preconnect"/>
<link crossorigin="" href="/fonts.gstatic.com" rel="preconnect"/>
<link crossorigin="" href="/fonts.googleapis.com" rel="preconnect"/>
<link href="../../../../../main.css" rel="stylesheet"/>

<noscript>

</noscript>
<link href="https://www.gstatic.com/devrel-devsite/prod/v3e2dbdc40e7394635e5230ecc02cb28039ea55a5d72db9939d2fb9fc9e16d0ff/tensorflow/images/favicon.png" rel="shortcut icon"/>
<link href="https://www.gstatic.com/devrel-devsite/prod/v3e2dbdc40e7394635e5230ecc02cb28039ea55a5d72db9939d2fb9fc9e16d0ff/tensorflow/images/apple-touch-icon-180x180.png" rel="apple-touch-icon"/><link href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy" rel="canonical"/><link href="https://www.tensorflow.org/s/opensearch.xml" rel="search" title="TensorFlow" type="application/opensearchdescription+xml"/>
<title>tf.distribute.experimental.MultiWorkerMirroredStrategy</title>
<meta content="tf.distribute.experimental.MultiWorkerMirroredStrategy" property="og:title"/>
<meta content="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy" property="og:url"/>
<meta content="en" property="og:locale"/>

</head>
<body class="" layout="docs" pending="" theme="tensorflow-theme" type="reference">
<devsite-progress id="app-progress" type="indeterminate"></devsite-progress>
<section class="devsite-wrapper"> <devsite-book-nav scrollbars="">

</devsite-book-nav>
<section id="gc-wrapper">
<main class="devsite-main-content" has-book-nav="" has-toc="" role="main">
<devsite-toc class="devsite-nav"></devsite-toc>
<devsite-content>
<article class="devsite-article">
<article class="devsite-article-inner"><style>
        /* Styles inlined from /site-assets/css/style.css */
/* override theme */
table img {
  max-width: 100%;
}

/* override var element to differentiate color from comment */
var, var code, var span, .prettyprint var span {
  color: #039be5;
}

/* .devsite-terminal virtualenv prompt */
.tfo-terminal-venv::before {
  content: "(venv) $ " !important;
}

/* .devsite-terminal root prompt */
.tfo-terminal-root::before {
  content: "# " !important;
}

/* .devsite-terminal Windows prompt */
.tfo-terminal-windows::before {
  content: "C:\\> " !important;
}

/* .devsite-terminal Windows prompt w/ virtualenv */
.tfo-terminal-windows-venv::before {
  content: "(venv) C:\\> " !important;
}

.tfo-diff-green-one-level + * {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-green + * > * {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-green-list + ul > li:first-of-type {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-red-one-level + * {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

.tfo-diff-red + * > * {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

.tfo-diff-red-list + ul > li:first-of-type {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

devsite-code .tfo-notebook-code-cell-output {
  max-height: 300px;
  overflow: auto;
  background: rgba(255, 247, 237, 1);  /* orange bg to distinguish from input code cells */
}

devsite-code .tfo-notebook-code-cell-output + .devsite-code-buttons-container button {
  background: rgba(255, 247, 237, .7);  /* orange bg to distinguish from input code cells */
}

devsite-code[dark-code] .tfo-notebook-code-cell-output {
  background: rgba(64, 78, 103, 1);  /* medium slate */
}

devsite-code[dark-code] .tfo-notebook-code-cell-output + .devsite-code-buttons-container button {
  background: rgba(64, 78, 103, .7);  /* medium slate */
}

/* override default table styles for notebook buttons */
.devsite-table-wrapper .tfo-notebook-buttons {
  display: inline-block;
  margin-left: 3px;
  width: auto;
}

.tfo-notebook-buttons td {
  padding-left: 0;
  padding-right: 20px;
}

.tfo-notebook-buttons a,
.tfo-notebook-buttons :link,
.tfo-notebook-buttons :visited {
  border-radius: 8px;
  box-shadow: 0 1px 2px 0 rgba(60, 64, 67, .3), 0 1px 3px 1px rgba(60, 64, 67, .15);
  color: #202124;
  padding: 12px 24px;
  transition: box-shadow 0.2s;
}

.tfo-notebook-buttons a:hover,
.tfo-notebook-buttons a:focus {
  box-shadow: 0 1px 2px 0 rgba(60, 64, 67, .3), 0 2px 6px 2px rgba(60, 64, 67, .15);
}

.tfo-notebook-buttons tr {
  background: 0;
  border: 0;
}

/* on rendered notebook page,
   remove link to webpage since we're already here */
.tfo-notebook-buttons:not(.tfo-api) td:first-child {
  display: none;
}

.tfo-notebook-buttons td > a {
  -webkit-box-align: center;
  -ms-flex-align: center;
  align-items: center;
  display: -webkit-box;
  display: -ms-flexbox;
  display: flex;
}

.tfo-notebook-buttons td > a > img {
  margin-right: 8px;
}

/* landing pages */

.tfo-landing-row-item-inset-white {
  background-color: #fff;
  padding: 32px;
}

.tfo-landing-row-item-inset-white ol,
.tfo-landing-row-item-inset-white ul {
  padding-left: 20px;
}

/* colab callout button */
.colab-callout-row devsite-code {
  border-radius: 8px 8px 0 0;
  box-shadow: none;
}

.colab-callout-footer {
  background: #e3e4e7;
  border-radius: 0 0 8px 8px;
  color: #37474f;
  padding: 20px;
}

.colab-callout-row devsite-code[dark-code] + .colab-callout-footer {
  background: #3f4f66;
}


.colab-callout-footer > .button {
  margin-top: 4px;
  color: #ff5c00;
}

.colab-callout-footer > a > span {
  padding-top: 10px;
  vertical-align: middle;
  color: #37474f;
  padding-left: 10px;
  padding-right: 10px;
  font-size: 14px;
}

.colab-callout-row devsite-code[dark-code] + .colab-callout-footer > a > span {
  color: #fff;
}

a.colab-button {
  background: rgba(255, 255, 255, .75);
  border: solid 1px rgba(0, 0, 0, .08);
  border-bottom-color: rgba(0, 0, 0, .15);
  border-radius: 4px;
  color: #aaa;
  display: inline-block;
  font-size: 11px !important;
  font-weight: 300;
  line-height: 16px;
  padding: 4px 8px;
  text-decoration: none;
  text-transform: uppercase;
}

a.colab-button:hover {
  background: white;
  border-color: rgba(0, 0, 0, .2);
  color: #666;
}

a.colab-button span {
  background: url(/images/colab_logo_button.svg) no-repeat 1px 1px / 20px;
  border-radius: 4px;
  display: inline-block;
  padding-left: 24px;
  text-decoration: none;
}

@media screen and (max-width: 600px) {
  .tfo-notebook-buttons td {
    display: block;
  }
}

/* guide and tutorials landing page cards and sections */

.tfo-landing-page-card {
  padding: 16px;
  box-shadow: 0 0 36px rgba(0,0,0,0.1);
  border-radius: 10px;
}

/* Page section headings */
.tfo-landing-page-heading h2, h2.tfo-landing-page-heading {
  font-family: "Google Sans", sans-serif;
  color: #425066;
  font-size: 30px;
  font-weight: 700;
  line-height: 40px;
}

/* Item title headings */
.tfo-landing-page-heading h3, h3.tfo-landing-page-heading,
.tfo-landing-page-card h3, h3.tfo-landing-page-card {
  font-family: "Google Sans", sans-serif;
  color: #425066;
  font-size: 20px;
  font-weight: 500;
  line-height: 26px;
}

/* List of tutorials notebooks for subsites */
.tfo-landing-page-resources-ul {
  padding-left: 15px
}

.tfo-landing-page-resources-ul > li {
  margin: 6px 0;
}

/* Temporary fix to hide product description in header on landing pages */
devsite-header .devsite-product-description {
  display: none;
}

        </style> <div class="devsite-banner devsite-banner-announcement">
<div class="devsite-banner-message">
<div class="devsite-banner-message-text">
            Missed TensorFlow Dev Summit? Check out the video playlist. <a class="button button-primary button-tfo-announcement" href="https://goo.gle/TFDS20AllSessions">Watch recordings</a>
</div>
</div>
</div>
<div class="devsite-article-meta">
<ul class="devsite-breadcrumb-list">
<li class="devsite-breadcrumb-item">
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="1" href="">
            TensorFlow
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="2" href="api">
            API
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="3" href="api_docs">
            TensorFlow Core v2.1.0
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="4" href="api_docs/python/tf">
            Python
      
  </a>
</li>
</ul>
<devsite-page-rating hover-rating-star="0" position="header" selected-rating="0">
</devsite-page-rating>
</div>
<a class="dashingAutolink" name="autolink-1283"></a><a class="dashAnchor" name="//apple_ref/cpp/Function/tf.distribute.experimental.MultiWorkerMirroredStrategy"></a><h1 class="dash-function">tf.distribute.experimental.MultiWorkerMirroredStrategy</h1>
<devsite-toc class="devsite-nav" devsite-toc-embedded="">
</devsite-toc>
<div class="devsite-article-body clearfix">
<p></p>
<!-- DO NOT EDIT! Automatically generated file. -->
<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<meta content="tf.distribute.experimental.MultiWorkerMirroredStrategy" itemprop="name"/>
<meta content="Stable" itemprop="path"/>
<meta content="__init__" itemprop="property"/>
<meta content="experimental_distribute_dataset" itemprop="property"/>
<meta content="experimental_distribute_datasets_from_function" itemprop="property"/>
<meta content="experimental_local_results" itemprop="property"/>
<meta content="experimental_make_numpy_dataset" itemprop="property"/>
<meta content="experimental_run_v2" itemprop="property"/>
<meta content="reduce" itemprop="property"/>
<meta content="scope" itemprop="property"/>
</div>
<p><devsite-nav-buttons name="version" param="reset">
<button default="" value="stable">See Stable</button>
<button value="nightly">See Nightly</button>
</devsite-nav-buttons></p>
<!-- Stable -->
<table align="left" class="tfo-notebook-buttons tfo-api">
<tbody><tr><td>
<a href="versions/r1.15/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy" target="_blank">
<img src="https://www.tensorflow.org/images/tf_logo_32px.png"/>
  TensorFlow 1 version</a>
</td>
<td>
<a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/distribute/collective_all_reduce_strategy.py#L48-L134" target="_blank">
<img src="https://www.tensorflow.org/images/GitHub-Mark-32px.png"/>
    View source on GitHub
  </a>
</td></tr></tbody></table>
<p>A distribution strategy for synchronous training on multiple workers.</p>
<p>Inherits From: <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"><code dir="ltr" translate="no">Strategy</code></a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">tf.distribute.experimental.MultiWorkerMirroredStrategy(
    communication=tf.distribute.experimental.CollectiveCommunication.AUTO,
    cluster_resolver=None
)
</code></pre>
<h3>Used in the notebooks</h3>
<table class="vertical-rules">
<thead>
<tr>
<th>Used in the guide</th>
<th>Used in the tutorials</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<ul>
<li><a href="https://www.tensorflow.org/guide/distributed_training">Distributed training with TensorFlow</a></li>
</ul>
</td>
<td>
<ul>
<li><a href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator">Multi-worker training with Estimator</a></li>
<li><a href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras">Multi-worker training with Keras</a></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>This strategy implements synchronous distributed training across multiple
workers, each with potentially multiple GPUs. Similar to
<a href="https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy"><code dir="ltr" translate="no">tf.distribute.MirroredStrategy</code></a>, it creates copies of all variables in the
model on each device across all workers.</p>
<p>It uses CollectiveOps&#39;s implementation of multi-worker all-reduce to
to keep variables in sync. A collective op is a single op in the
TensorFlow graph which can automatically choose an all-reduce algorithm in
the TensorFlow runtime according to hardware, network topology and tensor
sizes.</p>
<p>By default it uses all local GPUs or CPU for single-worker training.</p>
<p>When &#39;TF_CONFIG&#39; environment variable is set, it parses cluster_spec,
task_type and task_id from &#39;TF_CONFIG&#39; and turns into a multi-worker strategy
which mirrores models on GPUs of all machines in a cluster. In the current
implementation, it uses all GPUs in a cluster and it assumes all workers have
the same number of GPUs.</p>
<p>You can also pass a <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver"><code dir="ltr" translate="no">distribute.cluster_resolver.ClusterResolver</code></a> instance
when instantiating the strategy. The task_type, task_id etc. will be parsed
from the resolver instance instead of from the <code dir="ltr" translate="no">TF_CONFIG</code> env var.</p>
<p>It supports both eager mode and graph mode. However, for eager mode, it has to
set up the eager context in its constructor and therefore all ops in eager
mode have to run after the strategy object is created.</p>
<h4 id="args_10">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">communication</code></b>: optional Enum of type
<a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CollectiveCommunication"><code dir="ltr" translate="no">distribute.experimental.CollectiveCommunication</code></a>.  This provides a way
for the user to override the choice of collective op communication.
Possible values include <code dir="ltr" translate="no">AUTO</code>, <code dir="ltr" translate="no">RING</code>, and <code dir="ltr" translate="no">NCCL</code>.</li>
<li><b><code dir="ltr" translate="no">cluster_resolver</code></b>: optional <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver"><code dir="ltr" translate="no">distribute.cluster_resolver.ClusterResolver</code></a>
object. The default ClusterResolver that is used is the
TFConfigClusterResolver which is instantiated from the TF_CONFIG env
var.</li>
</ul>
<h4 id="attributes_2">Attributes:</h4>
<ul>
<li><b><code dir="ltr" translate="no">extended</code></b>:   <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended"><code dir="ltr" translate="no">tf.distribute.StrategyExtended</code></a> with additional methods.</li>
<li><b><code dir="ltr" translate="no">num_replicas_in_sync</code></b>:   Returns number of replicas over which gradients are aggregated.</li>
</ul>
<h2 id="methods_2">Methods</h2>
<h3 id="experimental_distribute_dataset"><code dir="ltr" translate="no">experimental_distribute_dataset</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/distribute/distribute_lib.py#L613-L677" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">experimental_distribute_dataset(
    dataset
)
</code></pre>
<p>Distributes a tf.data.Dataset instance provided via <code dir="ltr" translate="no">dataset</code>.</p>
<p>The returned distributed dataset can be iterated over similar to how
regular datasets can.
NOTE: Currently, the user cannot add any more transformations to a
distributed dataset.</p>
<p>The following is an example:</p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">strategy = tf.distribute.MirroredStrategy()

# Create a dataset
dataset = dataset_ops.Dataset.TFRecordDataset([
  &#34;/a/1.tfr&#34;, &#34;/a/2.tfr&#34;, &#34;/a/3.tfr&#34;, &#34;/a/4.tfr&#34;])

# Distribute that dataset
dist_dataset = strategy.experimental_distribute_dataset(dataset)
# Iterate over the distributed dataset
for x in dist_dataset:
  # process dataset elements
  strategy.experimental_run_v2(train_step, args=(x,))
</code></pre>
<p>We will assume that the input dataset is batched by the
global batch size. With this assumption, we will make a best effort to
divide each batch across all the replicas (one or more workers).</p>
<p>In a multi-worker setting, we will first attempt to distribute the dataset
by attempting to detect whether the dataset is being created out of
ReaderDatasets (e.g. TFRecordDataset, TextLineDataset, etc.) and if so,
attempting to shard the input files. Note that there has to be at least one
input file per worker. If you have less than one input file per worker, we
suggest that you should disable distributing your dataset using the method
below.</p>
<p>If that attempt is unsuccessful (e.g. the dataset is created from a
Dataset.range), we will shard the dataset evenly at the end by appending a
<code dir="ltr" translate="no">.shard</code> operation to the end of the processing pipeline. This will cause
the entire preprocessing pipeline for all the data to be run on every
worker, and each worker will do redundant work. We will print a warning
if this method of sharding is selected. In this case, consider using
<code dir="ltr" translate="no">experimental_distribute_datasets_from_function</code> instead.</p>
<p>You can disable dataset sharding across workers using the <code dir="ltr" translate="no">auto_shard</code>
option in <a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions"><code dir="ltr" translate="no">tf.data.experimental.DistributeOptions</code></a>.</p>
<p>Within each worker, we will also split the data among all the worker
devices (if more than one a present), and this will happen even if
multi-worker sharding is disabled using the method above.</p>
<p>If the above batch splitting and dataset sharding logic is undesirable,
please use <code dir="ltr" translate="no">experimental_distribute_datasets_from_function</code> instead, which
does not do any automatic splitting or sharding.</p>
<h4 id="args_11">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">dataset</code></b>: <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code dir="ltr" translate="no">tf.data.Dataset</code></a> that will be sharded across all replicas using
the rules stated above.</li>
</ul>
<h4 id="returns_12">Returns:</h4>
<p>A &#34;distributed <code dir="ltr" translate="no">Dataset</code>&#34;, which acts like a <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code dir="ltr" translate="no">tf.data.Dataset</code></a> except
it produces &#34;per-replica&#34; values.</p>
<h3 id="experimental_distribute_datasets_from_function"><code dir="ltr" translate="no">experimental_distribute_datasets_from_function</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/distribute/distribute_lib.py#L679-L727" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">experimental_distribute_datasets_from_function(
    dataset_fn
)
</code></pre>
<p>Distributes <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code dir="ltr" translate="no">tf.data.Dataset</code></a> instances created by calls to <code dir="ltr" translate="no">dataset_fn</code>.</p>
<p><code dir="ltr" translate="no">dataset_fn</code> will be called once for each worker in the strategy. Each
replica on that worker will dequeue one batch of inputs from the local
<code dir="ltr" translate="no">Dataset</code> (i.e. if a worker has two replicas, two batches will be dequeued
from the <code dir="ltr" translate="no">Dataset</code> every step).</p>
<p>This method can be used for several purposes. For example, where
<code dir="ltr" translate="no">experimental_distribute_dataset</code> is unable to shard the input files, this
method might be used to manually shard the dataset (avoiding the slow
fallback behavior in <code dir="ltr" translate="no">experimental_distribute_dataset</code>). In cases where the
dataset is infinite, this sharding can be done by creating dataset replicas
that differ only in their random seed.
<code dir="ltr" translate="no">experimental_distribute_dataset</code> may also sometimes fail to split the
batch across replicas on a worker. In that case, this method can be used
where that limitation does not exist.</p>
<p>The <code dir="ltr" translate="no">dataset_fn</code> should take an <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext"><code dir="ltr" translate="no">tf.distribute.InputContext</code></a> instance where
information about batching and input replication can be accessed:</p>
<pre class="prettyprint" dir="ltr" translate="no"><code dir="ltr" translate="no">def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.experimental_distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.experimental_run_v2(replica_fn, args=(batch,))
</code></pre>
<p>IMPORTANT: The <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code dir="ltr" translate="no">tf.data.Dataset</code></a> returned by <code dir="ltr" translate="no">dataset_fn</code> should have a
per-replica batch size, unlike <code dir="ltr" translate="no">experimental_distribute_dataset</code>, which uses
the global batch size.  This may be computed using
<code dir="ltr" translate="no">input_context.get_per_replica_batch_size</code>.</p>
<h4 id="args_12">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">dataset_fn</code></b>: A function taking a <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext"><code dir="ltr" translate="no">tf.distribute.InputContext</code></a> instance and
returning a <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code dir="ltr" translate="no">tf.data.Dataset</code></a>.</li>
</ul>
<h4 id="returns_13">Returns:</h4>
<p>A &#34;distributed <code dir="ltr" translate="no">Dataset</code>&#34;, which acts like a <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code dir="ltr" translate="no">tf.data.Dataset</code></a> except
it produces &#34;per-replica&#34; values.</p>
<h3 id="experimental_local_results"><code dir="ltr" translate="no">experimental_local_results</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/distribute/distribute_lib.py#L886-L903" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">experimental_local_results(
    value
)
</code></pre>
<p>Returns the list of all local per-replica values contained in <code dir="ltr" translate="no">value</code>.</p>
<aside class="note"><strong>Note:</strong><span> This only returns values on the worker initiated by this client.
When using a <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"><code dir="ltr" translate="no">tf.distribute.Strategy</code></a> like
<a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy"><code dir="ltr" translate="no">tf.distribute.experimental.MultiWorkerMirroredStrategy</code></a>, each worker
will be its own client, and this function will only return values
computed on that worker.</span></aside>
<h4 id="args_13">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">value</code></b>: A value returned by <code dir="ltr" translate="no">experimental_run()</code>, <code dir="ltr" translate="no">experimental_run_v2()</code>,
<code dir="ltr" translate="no">extended.call_for_each_replica()</code>, or a variable created in <code dir="ltr" translate="no">scope</code>.</li>
</ul>
<h4 id="returns_14">Returns:</h4>
<p>A tuple of values contained in <code dir="ltr" translate="no">value</code>. If <code dir="ltr" translate="no">value</code> represents a single
value, this returns <code dir="ltr" translate="no">(value,).</code></p>
<h3 id="experimental_make_numpy_dataset"><code dir="ltr" translate="no">experimental_make_numpy_dataset</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/distribute/distribute_lib.py#L578-L604" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">experimental_make_numpy_dataset(
    numpy_input
)
</code></pre>
<p>Makes a <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code dir="ltr" translate="no">tf.data.Dataset</code></a> for input provided via a numpy array.</p>
<p>This avoids adding <code dir="ltr" translate="no">numpy_input</code> as a large constant in the graph,
and copies the data to the machine or machines that will be processing
the input.</p>
<p>Note that you will likely need to use <code dir="ltr" translate="no">experimental_distribute_dataset</code>
with the returned dataset to further distribute it with the strategy.</p>
<h4 id="example_2">Example:</h4>
<pre class="prettyprint" dir="ltr" translate="no"><code dir="ltr" translate="no">numpy_input = np.ones([10], dtype=np.float32)
dataset = strategy.experimental_make_numpy_dataset(numpy_input)
dist_dataset = strategy.experimental_distribute_dataset(dataset)
</code></pre>
<h4 id="args_14">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">numpy_input</code></b>: A nest of NumPy input arrays that will be converted into a
dataset. Note that lists of Numpy arrays are stacked, as that is normal
<a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code dir="ltr" translate="no">tf.data.Dataset</code></a> behavior.</li>
</ul>
<h4 id="returns_15">Returns:</h4>
<p>A <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code dir="ltr" translate="no">tf.data.Dataset</code></a> representing <code dir="ltr" translate="no">numpy_input</code>.</p>
<h3 id="experimental_run_v2"><code dir="ltr" translate="no">experimental_run_v2</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/distribute/distribute_lib.py#L729-L763" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">experimental_run_v2(
    fn, args=(), kwargs=None
)
</code></pre>
<p>Run <code dir="ltr" translate="no">fn</code> on each replica, with the given arguments.</p>
<p>Executes ops specified by <code dir="ltr" translate="no">fn</code> on each replica. If <code dir="ltr" translate="no">args</code> or <code dir="ltr" translate="no">kwargs</code> have
&#34;per-replica&#34; values, such as those produced by a &#34;distributed <code dir="ltr" translate="no">Dataset</code>&#34;,
when <code dir="ltr" translate="no">fn</code> is executed on a particular replica, it will be executed with the
component of those &#34;per-replica&#34; values that correspond to that replica.</p>
<p><code dir="ltr" translate="no">fn</code> may call <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context"><code dir="ltr" translate="no">tf.distribute.get_replica_context()</code></a> to access members such
as <code dir="ltr" translate="no">all_reduce</code>.</p>
<p>All arguments in <code dir="ltr" translate="no">args</code> or <code dir="ltr" translate="no">kwargs</code> should either be nest of tensors or
per-replica objects containing tensors or composite tensors.</p>
<p>IMPORTANT: Depending on the implementation of <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"><code dir="ltr" translate="no">tf.distribute.Strategy</code></a> and
whether eager execution is enabled, <code dir="ltr" translate="no">fn</code> may be called one or more times (
once for each replica).</p>
<h4 id="args_15">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">fn</code></b>: The function to run. The output must be a <a href="https://www.tensorflow.org/api_docs/python/tf/nest"><code dir="ltr" translate="no">tf.nest</code></a> of <code dir="ltr" translate="no">Tensor</code>s.</li>
<li><b><code dir="ltr" translate="no">args</code></b>: (Optional) Positional arguments to <code dir="ltr" translate="no">fn</code>.</li>
<li><b><code dir="ltr" translate="no">kwargs</code></b>: (Optional) Keyword arguments to <code dir="ltr" translate="no">fn</code>.</li>
</ul>
<h4 id="returns_16">Returns:</h4>
<p>Merged return value of <code dir="ltr" translate="no">fn</code> across replicas. The structure of the return
value is the same as the return value from <code dir="ltr" translate="no">fn</code>. Each element in the
structure can either be &#34;per-replica&#34; <code dir="ltr" translate="no">Tensor</code> objects or <code dir="ltr" translate="no">Tensor</code>s
(for example, if running on a single replica).</p>
<h3 id="reduce"><code dir="ltr" translate="no">reduce</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/distribute/distribute_lib.py#L765-L862" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">reduce(
    reduce_op, value, axis
)
</code></pre>
<p>Reduce <code dir="ltr" translate="no">value</code> across replicas.</p>
<p>Given a per-replica value returned by <code dir="ltr" translate="no">experimental_run_v2</code>, say a
per-example loss, the batch will be divided across all the replicas.  This
function allows you to aggregate across replicas and optionally also across
batch elements.  For example, if you have a global batch size of 8 and 2
replicas, values for examples <code dir="ltr" translate="no">[0, 1, 2, 3]</code> will be on replica 0 and
<code dir="ltr" translate="no">[4, 5, 6, 7]</code> will be on replica 1. By default, <code dir="ltr" translate="no">reduce</code> will just
aggregate across replicas, returning <code dir="ltr" translate="no">[0+4, 1+5, 2+6, 3+7]</code>. This is useful
when each replica is computing a scalar or some other value that doesn&#39;t
have a &#34;batch&#34; dimension (like a gradient). More often you will want to
aggregate across the global batch, which you can get by specifying the batch
dimension as the <code dir="ltr" translate="no">axis</code>, typically <code dir="ltr" translate="no">axis=0</code>. In this case it would return a
scalar <code dir="ltr" translate="no">0+1+2+3+4+5+6+7</code>.</p>
<p>If there is a last partial batch, you will need to specify an axis so
that the resulting shape is consistent across replicas. So if the last
batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you
would get a shape mismatch unless you specify <code dir="ltr" translate="no">axis=0</code>. If you specify
<a href="https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp#MEAN"><code dir="ltr" translate="no">tf.distribute.ReduceOp.MEAN</code></a>, using <code dir="ltr" translate="no">axis=0</code> will use the correct
denominator of 6. Contrast this with computing <code dir="ltr" translate="no">reduce_mean</code> to get a
scalar value on each replica and this function to average those means,
which will weigh some values <code dir="ltr" translate="no">1/8</code> and others <code dir="ltr" translate="no">1/4</code>.</p>
<h4 id="args_16">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">reduce_op</code></b>: A <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp"><code dir="ltr" translate="no">tf.distribute.ReduceOp</code></a> value specifying how values should
be combined.</li>
<li><b><code dir="ltr" translate="no">value</code></b>: A &#34;per replica&#34; value, e.g. returned by <code dir="ltr" translate="no">experimental_run_v2</code> to
be combined into a single tensor.</li>
<li><b><code dir="ltr" translate="no">axis</code></b>: Specifies the dimension to reduce along within each
replica&#39;s tensor. Should typically be set to the batch dimension, or
<code dir="ltr" translate="no">None</code> to only reduce across replicas (e.g. if the tensor has no batch
dimension).</li>
</ul>
<h4 id="returns_17">Returns:</h4>
<p>A <code dir="ltr" translate="no">Tensor</code>.</p>
<h3 id="scope"><code dir="ltr" translate="no">scope</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/distribute/collective_all_reduce_strategy.py#L119-L134" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">scope()
</code></pre>
<p>Returns a context manager selecting this Strategy as current.</p>
<p>Inside a <code dir="ltr" translate="no">with strategy.scope():</code> code block, this thread
will use a variable creator set by <code dir="ltr" translate="no">strategy</code>, and will
enter its &#34;cross-replica context&#34;.</p>
<p>In <code dir="ltr" translate="no">MultiWorkerMirroredStrategy</code>, all variables created inside
`strategy.scope() will be mirrored on all replicas of each worker.
Moreover, it also sets a default device scope so that ops without
specified devices will end up on the correct worker.</p>
<h4 id="returns_18">Returns:</h4>
<p>A context manager to use for creating variables with this strategy.</p>
</div>
<devsite-page-rating hover-rating-star="0" position="footer" selected-rating="0">
</devsite-page-rating>
</article>
</article>

</devsite-content>
</main>
<devsite-footer-promos class="devsite-footer">
</devsite-footer-promos>
<devsite-footer-linkboxes class="devsite-footer">

</devsite-footer-linkboxes>
<devsite-footer-utility class="devsite-footer">
<div class="devsite-footer-utility nocontent">

</div>
</devsite-footer-utility>
</section></section>
<devsite-sitemask></devsite-sitemask>
<devsite-snackbar></devsite-snackbar> <devsite-tooltip></devsite-tooltip>
<devsite-heading-link></devsite-heading-link>
<devsite-analytics>


</devsite-analytics>
 
</body></html>