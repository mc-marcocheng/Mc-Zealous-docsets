<!DOCTYPE html><html dir="ltr" lang="en"><head>
<meta content="157101835696-ooapojlodmuabs2do2vuhhnf90bccmoi.apps.googleusercontent.com" name="google-signin-client-id"/>
<meta content="profile email" name="google-signin-scope"/>
<meta content="TensorFlow" property="og:site_name"/>
<meta content="website" property="og:type"/>
<meta content="#ff6f00" name="theme-color"/>
<meta charset="utf-8"/>
<meta content="IE=Edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<link crossorigin="use-credentials" href="_pwa/tensorflow/manifest.json" rel="manifest"/>
<link crossorigin="" href="/www.gstatic.com" rel="preconnect"/>
<link crossorigin="" href="/fonts.gstatic.com" rel="preconnect"/>
<link crossorigin="" href="/fonts.googleapis.com" rel="preconnect"/>
<link href="../../../main.css" rel="stylesheet"/>

<noscript>

</noscript>
<link href="https://www.gstatic.com/devrel-devsite/prod/v3e2dbdc40e7394635e5230ecc02cb28039ea55a5d72db9939d2fb9fc9e16d0ff/tensorflow/images/favicon.png" rel="shortcut icon"/>
<link href="https://www.gstatic.com/devrel-devsite/prod/v3e2dbdc40e7394635e5230ecc02cb28039ea55a5d72db9939d2fb9fc9e16d0ff/tensorflow/images/apple-touch-icon-180x180.png" rel="apple-touch-icon"/><link href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" rel="canonical"/><link href="https://www.tensorflow.org/s/opensearch.xml" rel="search" title="TensorFlow" type="application/opensearchdescription+xml"/>
<title>tf.GradientTape &nbsp;|&nbsp; TensorFlow Core v2.1.0</title>
<meta content="tf.GradientTape &nbsp;|&nbsp; TensorFlow Core v2.1.0" property="og:title"/>
<meta content="https://www.tensorflow.org/api_docs/python/tf/GradientTape" property="og:url"/>
<meta content="en" property="og:locale"/>

</head>
<body class="" layout="docs" pending="" theme="tensorflow-theme" type="reference">
<devsite-progress id="app-progress" type="indeterminate"></devsite-progress>
<section class="devsite-wrapper"> <devsite-book-nav scrollbars="">

</devsite-book-nav>
<section id="gc-wrapper">
<main class="devsite-main-content" has-book-nav="" has-toc="" role="main">
<devsite-toc class="devsite-nav"></devsite-toc>
<devsite-content>
<article class="devsite-article">
<article class="devsite-article-inner"><style>
        /* Styles inlined from /site-assets/css/style.css */
/* override theme */
table img {
  max-width: 100%;
}

/* override var element to differentiate color from comment */
var, var code, var span, .prettyprint var span {
  color: #039be5;
}

/* .devsite-terminal virtualenv prompt */
.tfo-terminal-venv::before {
  content: "(venv) $ " !important;
}

/* .devsite-terminal root prompt */
.tfo-terminal-root::before {
  content: "# " !important;
}

/* .devsite-terminal Windows prompt */
.tfo-terminal-windows::before {
  content: "C:\\> " !important;
}

/* .devsite-terminal Windows prompt w/ virtualenv */
.tfo-terminal-windows-venv::before {
  content: "(venv) C:\\> " !important;
}

.tfo-diff-green-one-level + * {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-green + * > * {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-green-list + ul > li:first-of-type {
  background: rgba(175, 245, 162, .6)  !important;
}

.tfo-diff-red-one-level + * {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

.tfo-diff-red + * > * {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

.tfo-diff-red-list + ul > li:first-of-type {
  background: rgba(255, 230, 230, .6)  !important;
  text-decoration: line-through  !important;
}

devsite-code .tfo-notebook-code-cell-output {
  max-height: 300px;
  overflow: auto;
  background: rgba(255, 247, 237, 1);  /* orange bg to distinguish from input code cells */
}

devsite-code .tfo-notebook-code-cell-output + .devsite-code-buttons-container button {
  background: rgba(255, 247, 237, .7);  /* orange bg to distinguish from input code cells */
}

devsite-code[dark-code] .tfo-notebook-code-cell-output {
  background: rgba(64, 78, 103, 1);  /* medium slate */
}

devsite-code[dark-code] .tfo-notebook-code-cell-output + .devsite-code-buttons-container button {
  background: rgba(64, 78, 103, .7);  /* medium slate */
}

/* override default table styles for notebook buttons */
.devsite-table-wrapper .tfo-notebook-buttons {
  display: inline-block;
  margin-left: 3px;
  width: auto;
}

.tfo-notebook-buttons td {
  padding-left: 0;
  padding-right: 20px;
}

.tfo-notebook-buttons a,
.tfo-notebook-buttons :link,
.tfo-notebook-buttons :visited {
  border-radius: 8px;
  box-shadow: 0 1px 2px 0 rgba(60, 64, 67, .3), 0 1px 3px 1px rgba(60, 64, 67, .15);
  color: #202124;
  padding: 12px 24px;
  transition: box-shadow 0.2s;
}

.tfo-notebook-buttons a:hover,
.tfo-notebook-buttons a:focus {
  box-shadow: 0 1px 2px 0 rgba(60, 64, 67, .3), 0 2px 6px 2px rgba(60, 64, 67, .15);
}

.tfo-notebook-buttons tr {
  background: 0;
  border: 0;
}

/* on rendered notebook page,
   remove link to webpage since we're already here */
.tfo-notebook-buttons:not(.tfo-api) td:first-child {
  display: none;
}

.tfo-notebook-buttons td > a {
  -webkit-box-align: center;
  -ms-flex-align: center;
  align-items: center;
  display: -webkit-box;
  display: -ms-flexbox;
  display: flex;
}

.tfo-notebook-buttons td > a > img {
  margin-right: 8px;
}

/* landing pages */

.tfo-landing-row-item-inset-white {
  background-color: #fff;
  padding: 32px;
}

.tfo-landing-row-item-inset-white ol,
.tfo-landing-row-item-inset-white ul {
  padding-left: 20px;
}

/* colab callout button */
.colab-callout-row devsite-code {
  border-radius: 8px 8px 0 0;
  box-shadow: none;
}

.colab-callout-footer {
  background: #e3e4e7;
  border-radius: 0 0 8px 8px;
  color: #37474f;
  padding: 20px;
}

.colab-callout-row devsite-code[dark-code] + .colab-callout-footer {
  background: #3f4f66;
}


.colab-callout-footer > .button {
  margin-top: 4px;
  color: #ff5c00;
}

.colab-callout-footer > a > span {
  padding-top: 10px;
  vertical-align: middle;
  color: #37474f;
  padding-left: 10px;
  padding-right: 10px;
  font-size: 14px;
}

.colab-callout-row devsite-code[dark-code] + .colab-callout-footer > a > span {
  color: #fff;
}

a.colab-button {
  background: rgba(255, 255, 255, .75);
  border: solid 1px rgba(0, 0, 0, .08);
  border-bottom-color: rgba(0, 0, 0, .15);
  border-radius: 4px;
  color: #aaa;
  display: inline-block;
  font-size: 11px !important;
  font-weight: 300;
  line-height: 16px;
  padding: 4px 8px;
  text-decoration: none;
  text-transform: uppercase;
}

a.colab-button:hover {
  background: white;
  border-color: rgba(0, 0, 0, .2);
  color: #666;
}

a.colab-button span {
  background: url(/images/colab_logo_button.svg) no-repeat 1px 1px / 20px;
  border-radius: 4px;
  display: inline-block;
  padding-left: 24px;
  text-decoration: none;
}

@media screen and (max-width: 600px) {
  .tfo-notebook-buttons td {
    display: block;
  }
}

/* guide and tutorials landing page cards and sections */

.tfo-landing-page-card {
  padding: 16px;
  box-shadow: 0 0 36px rgba(0,0,0,0.1);
  border-radius: 10px;
}

/* Page section headings */
.tfo-landing-page-heading h2, h2.tfo-landing-page-heading {
  font-family: "Google Sans", sans-serif;
  color: #425066;
  font-size: 30px;
  font-weight: 700;
  line-height: 40px;
}

/* Item title headings */
.tfo-landing-page-heading h3, h3.tfo-landing-page-heading,
.tfo-landing-page-card h3, h3.tfo-landing-page-card {
  font-family: "Google Sans", sans-serif;
  color: #425066;
  font-size: 20px;
  font-weight: 500;
  line-height: 26px;
}

/* List of tutorials notebooks for subsites */
.tfo-landing-page-resources-ul {
  padding-left: 15px
}

.tfo-landing-page-resources-ul > li {
  margin: 6px 0;
}

/* Temporary fix to hide product description in header on landing pages */
devsite-header .devsite-product-description {
  display: none;
}

        </style> <div class="devsite-banner devsite-banner-announcement">
<div class="devsite-banner-message">
<div class="devsite-banner-message-text">
            Missed TensorFlow Dev Summit? Check out the video playlist. <a class="button button-primary button-tfo-announcement" href="https://goo.gle/TFDS20AllSessions">Watch recordings</a>
</div>
</div>
</div>
<div class="devsite-article-meta">
<ul class="devsite-breadcrumb-list">
<li class="devsite-breadcrumb-item">
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="1" href="">
            TensorFlow
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="2" href="api">
            API
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="3" href="api_docs">
            TensorFlow Core v2.1.0
      
  </a>
</li>
<li class="devsite-breadcrumb-item">
<div aria-hidden="true" class="devsite-breadcrumb-guillemet material-icons"></div>
<a class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="4" href="api_docs/python/tf">
            Python
      
  </a>
</li>
</ul>
<devsite-page-rating hover-rating-star="0" position="header" selected-rating="0">
</devsite-page-rating>
</div>
<a class="dashingAutolink" name="autolink-5"></a><a class="dashAnchor" name="//apple_ref/cpp/Class/tf.GradientTape"></a><h1 class="dash-class">tf.GradientTape</h1>
<devsite-toc class="devsite-nav" devsite-toc-embedded="">
</devsite-toc>
<div class="devsite-article-body clearfix">
<p></p>
<!-- DO NOT EDIT! Automatically generated file. -->
<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<meta content="tf.GradientTape" itemprop="name"/>
<meta content="Stable" itemprop="path"/>
<meta content="__enter__" itemprop="property"/>
<meta content="__exit__" itemprop="property"/>
<meta content="__init__" itemprop="property"/>
<meta content="batch_jacobian" itemprop="property"/>
<meta content="gradient" itemprop="property"/>
<meta content="jacobian" itemprop="property"/>
<meta content="reset" itemprop="property"/>
<meta content="stop_recording" itemprop="property"/>
<meta content="watch" itemprop="property"/>
<meta content="watched_variables" itemprop="property"/>
</div>
<!-- Insert buttons and diff -->
<table align="left" class="tfo-notebook-buttons tfo-api">
<tbody><tr><td>
<a href="versions/r1.15/api_docs/python/tf/GradientTape" target="_blank">
<img src="https://www.tensorflow.org/images/tf_logo_32px.png"/>
  TensorFlow 1 version</a>
</td>
<td>
<a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/eager/backprop.py#L702-L1262" target="_blank">
<img src="https://www.tensorflow.org/images/GitHub-Mark-32px.png"/>
    View source on GitHub
  </a>
</td></tr></tbody></table>
<p>Record operations for automatic differentiation.</p>
<section class="expandable">
<h4 class="showalways">View aliases</h4>
<p>
<b>Main aliases</b>
</p><p><a href="api_docs/python/tf/GradientTape"><code dir="ltr" translate="no">tf.autodiff.GradientTape</code></a></p>
<b>Compat aliases for migration</b>
<p>See
<a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for
more details.</p>
<p><a href="api_docs/python/tf/GradientTape"><code dir="ltr" translate="no">tf.compat.v1.GradientTape</code></a></p>
</section>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">tf.GradientTape(
    persistent=False, watch_accessed_variables=True
)
</code></pre>
<h3>Used in the notebooks</h3>
<table class="vertical-rules">
<thead>
<tr>
<th>Used in the guide</th>
<th>Used in the tutorials</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<ul>
<li><a href="https://www.tensorflow.org/guide/eager">Eager execution</a></li>
<li><a href="https://www.tensorflow.org/guide/keras/train_and_evaluate">Train and evaluate with Keras</a></li>
<li><a href="https://www.tensorflow.org/guide/checkpoint">Training checkpoints</a></li>
<li><a href="https://www.tensorflow.org/guide/migrate">Migrate your TensorFlow 1 code to TensorFlow 2</a></li>
<li><a href="https://www.tensorflow.org/guide/distributed_training">Distributed training with TensorFlow</a></li>
</ul>
</td>
<td>
<ul>
<li><a href="https://www.tensorflow.org/tutorials/customization/autodiff">Automatic differentiation and gradient tape</a></li>
<li><a href="https://www.tensorflow.org/tutorials/generative/dcgan">Deep Convolutional Generative Adversarial Network</a></li>
<li><a href="https://www.tensorflow.org/tutorials/generative/deepdream">DeepDream</a></li>
<li><a href="https://www.tensorflow.org/tutorials/generative/pix2pix">Pix2Pix</a></li>
<li><a href="https://www.tensorflow.org/tutorials/generative/style_transfer">Neural style transfer</a></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Operations are recorded if they are executed within this context manager and
at least one of their inputs is being &#34;watched&#34;.</p>
<p>Trainable variables (created by <a href="https://www.tensorflow.org/api_docs/python/tf/Variable"><code dir="ltr" translate="no">tf.Variable</code></a> or <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable"><code dir="ltr" translate="no">tf.compat.v1.get_variable</code></a>,
where <code dir="ltr" translate="no">trainable=True</code> is default in both cases) are automatically watched.
Tensors can be manually watched by invoking the <code dir="ltr" translate="no">watch</code> method on this context
manager.</p>
<p>For example, consider the function <code dir="ltr" translate="no">y = x * x</code>. The gradient at <code dir="ltr" translate="no">x = 3.0</code> can
be computed as:</p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x) # Will compute to 6.0
</code></pre>
<p>GradientTapes can be nested to compute higher-order derivatives. For example,</p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  with tf.GradientTape() as gg:
    gg.watch(x)
    y = x * x
  dy_dx = gg.gradient(y, x)     # Will compute to 6.0
d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0
</code></pre>
<p>By default, the resources held by a GradientTape are released as soon as
GradientTape.gradient() method is called. To compute multiple gradients over
the same computation, create a persistent gradient tape. This allows multiple
calls to the gradient() method as resources are released when the tape object
is garbage collected. For example:</p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as g:
  g.watch(x)
  y = x * x
  z = y * y
dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)
dy_dx = g.gradient(y, x)  # 6.0
del g  # Drop the reference to the tape
</code></pre>
<p>By default GradientTape will automatically watch any trainable variables that
are accessed inside the context. If you want fine grained control over which
variables are watched you can disable automatic tracking by passing
<code dir="ltr" translate="no">watch_accessed_variables=False</code> to the tape constructor:</p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">with tf.GradientTape(watch_accessed_variables=False) as tape:
  tape.watch(variable_a)
  y = variable_a ** 2  # Gradients will be available for `variable_a`.
  z = variable_b ** 3  # No gradients will be available since `variable_b` is
                       # not being watched.
</code></pre>
<p>Note that when using models you should ensure that your variables exist when
using <code dir="ltr" translate="no">watch_accessed_variables=False</code>. Otherwise it&#39;s quite easy to make your
first iteration not have any gradients:</p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">a = tf.keras.layers.Dense(32)
b = tf.keras.layers.Dense(32)

with tf.GradientTape(watch_accessed_variables=False) as tape:
  tape.watch(a.variables)  # Since `a.build` has not been called at this point
                           # `a.variables` will return an empty list and the
                           # tape will not be watching anything.
  result = b(a(inputs))
  tape.gradient(result, a.variables)  # The result of this computation will be
                                      # a list of `None`s since a&#39;s variables
                                      # are not being watched.
</code></pre>
<p>Note that only tensors with real or complex dtypes are differentiable.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">persistent</code></b>: Boolean controlling whether a persistent gradient tape
is created. False by default, which means at most one call can
be made to the gradient() method on this object.</li>
<li><b><code dir="ltr" translate="no">watch_accessed_variables</code></b>: Boolean controlling whether the tape will
automatically <code dir="ltr" translate="no">watch</code> any (trainable) variables accessed while the tape
is active. Defaults to True meaning gradients can be requested from any
result computed in the tape derived from reading a trainable <code dir="ltr" translate="no">Variable</code>.
If False users must explicitly <code dir="ltr" translate="no">watch</code> any <code dir="ltr" translate="no">Variable</code>s they want to
request gradients from.</li>
</ul>
<h2 id="methods">Methods</h2>
<h3 id="__enter__"><code dir="ltr" translate="no">__enter__</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/eager/backprop.py#L812-L815" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">__enter__()
</code></pre>
<p>Enters a context inside which operations are recorded on this tape.</p>
<h3 id="__exit__"><code dir="ltr" translate="no">__exit__</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/eager/backprop.py#L817-L820" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">__exit__(
    typ, value, traceback
)
</code></pre>
<p>Exits the recording context, no further operations are traced.</p>
<h3 id="batch_jacobian"><code dir="ltr" translate="no">batch_jacobian</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/eager/backprop.py#L1143-L1262" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">batch_jacobian(
    target, source, unconnected_gradients=tf.UnconnectedGradients.NONE,
    parallel_iterations=None, experimental_use_pfor=True
)
</code></pre>
<p>Computes and stacks per-example jacobians.</p>
<p>See <a href="http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant">wikipedia article</a> for the
definition of a Jacobian. This function is essentially an efficient
implementation of the following:</p>
<p><code dir="ltr" translate="no">tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])</code>.</p>
<p>Note that compared to <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape#jacobian"><code dir="ltr" translate="no">GradientTape.jacobian</code></a> which computes gradient of
each output value w.r.t each input value, this function is useful when
<code dir="ltr" translate="no">target[i,...]</code> is independent of <code dir="ltr" translate="no">source[j,...]</code> for <code dir="ltr" translate="no">j != i</code>. This
assumption allows more efficient computation as compared to
<a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape#jacobian"><code dir="ltr" translate="no">GradientTape.jacobian</code></a>. The output, as well as intermediate activations,
are lower dimensional and avoid a bunch of redundant zeros which would
result in the jacobian computation given the independence assumption.</p>
<h4 id="example_usage">Example usage:</h4>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">with tf.GradientTape() as g:
  x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)
  g.watch(x)
  y = x * x
batch_jacobian = g.batch_jacobian(y, x)
# batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]
</code></pre>
<h4 id="args_2">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">target</code></b>: A tensor with rank 2 or higher and with shape [b, y1, ..., y_n].
<code dir="ltr" translate="no">target[i,...]</code> should only depend on <code dir="ltr" translate="no">source[i,...]</code>.</li>
<li><b><code dir="ltr" translate="no">source</code></b>: A tensor with rank 2 or higher and with shape [b, x1, ..., x_m].</li>
<li><b><code dir="ltr" translate="no">unconnected_gradients</code></b>: a value which can either hold &#39;none&#39; or &#39;zero&#39; and
alters the value which will be returned if the target and sources are
unconnected. The possible values and effects are detailed in
&#39;UnconnectedGradients&#39; and it defaults to &#39;none&#39;.</li>
<li><b><code dir="ltr" translate="no">parallel_iterations</code></b>: A knob to control how many iterations are dispatched
in parallel. This knob can be used to control the total memory usage.</li>
<li><b><code dir="ltr" translate="no">experimental_use_pfor</code></b>: If true, uses pfor for computing the Jacobian. Else
uses a tf.while_loop.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A tensor <code dir="ltr" translate="no">t</code> with shape [b, y_1, ..., y_n, x1, ..., x_m] where <code dir="ltr" translate="no">t[i, ...]</code>
is the jacobian of <code dir="ltr" translate="no">target[i, ...]</code> w.r.t. <code dir="ltr" translate="no">source[i, ...]</code>, i.e. stacked
per-example jacobians.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code dir="ltr" translate="no">RuntimeError</code></b>: If called on a non-persistent tape with eager execution
enabled and without enabling experimental_use_pfor.</li>
<li><b><code dir="ltr" translate="no">ValueError</code></b>: If vectorization of jacobian computation fails or if first
dimension of <code dir="ltr" translate="no">target</code> and <code dir="ltr" translate="no">source</code> do not match.</li>
</ul>
<h3 id="gradient"><code dir="ltr" translate="no">gradient</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/eager/backprop.py#L949-L1037" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">gradient(
    target, sources, output_gradients=None,
    unconnected_gradients=tf.UnconnectedGradients.NONE
)
</code></pre>
<p>Computes the gradient using operations recorded in context of this tape.</p>
<h4 id="args_3">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">target</code></b>: a list or nested structure of Tensors or Variables to be
differentiated.</li>
<li><b><code dir="ltr" translate="no">sources</code></b>: a list or nested structure of Tensors or Variables. <code dir="ltr" translate="no">target</code>
will be differentiated against elements in <code dir="ltr" translate="no">sources</code>.</li>
<li><b><code dir="ltr" translate="no">output_gradients</code></b>: a list of gradients, one for each element of
target. Defaults to None.</li>
<li><b><code dir="ltr" translate="no">unconnected_gradients</code></b>: a value which can either hold &#39;none&#39; or &#39;zero&#39; and
alters the value which will be returned if the target and sources are
unconnected. The possible values and effects are detailed in
&#39;UnconnectedGradients&#39; and it defaults to &#39;none&#39;.</li>
</ul>
<h4 id="returns_2">Returns:</h4>
<p>a list or nested structure of Tensors (or IndexedSlices, or None),
one for each element in <code dir="ltr" translate="no">sources</code>. Returned structure is the same as
the structure of <code dir="ltr" translate="no">sources</code>.</p>
<h4 id="raises_2">Raises:</h4>
<ul>
<li><b><code dir="ltr" translate="no">RuntimeError</code></b>: if called inside the context of the tape, or if called more
than once on a non-persistent tape.</li>
<li><b><code dir="ltr" translate="no">ValueError</code></b>: if the target is a variable or if unconnected gradients is
called with an unknown value.</li>
</ul>
<h3 id="jacobian"><code dir="ltr" translate="no">jacobian</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/eager/backprop.py#L1039-L1141" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">jacobian(
    target, sources, unconnected_gradients=tf.UnconnectedGradients.NONE,
    parallel_iterations=None, experimental_use_pfor=True
)
</code></pre>
<p>Computes the jacobian using operations recorded in context of this tape.</p>
<p>See <a href="http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant">wikipedia article</a> for the
definition of a Jacobian.</p>
<h4 id="example_usage_2">Example usage:</h4>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">with tf.GradientTape() as g:
  x  = tf.constant([1.0, 2.0])
  g.watch(x)
  y = x * x
jacobian = g.jacobian(y, x)
# jacobian value is [[2., 0.], [0., 4.]]
</code></pre>
<h4 id="args_4">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">target</code></b>: Tensor to be differentiated.</li>
<li><b><code dir="ltr" translate="no">sources</code></b>: a list or nested structure of Tensors or Variables. <code dir="ltr" translate="no">target</code>
will be differentiated against elements in <code dir="ltr" translate="no">sources</code>.</li>
<li><b><code dir="ltr" translate="no">unconnected_gradients</code></b>: a value which can either hold &#39;none&#39; or &#39;zero&#39; and
alters the value which will be returned if the target and sources are
unconnected. The possible values and effects are detailed in
&#39;UnconnectedGradients&#39; and it defaults to &#39;none&#39;.</li>
<li><b><code dir="ltr" translate="no">parallel_iterations</code></b>: A knob to control how many iterations are dispatched
in parallel. This knob can be used to control the total memory usage.</li>
<li><b><code dir="ltr" translate="no">experimental_use_pfor</code></b>: If true, vectorizes the jacobian computation. Else
falls back to a sequential while_loop. Vectorization can sometimes fail
or lead to excessive memory usage. This option can be used to disable
vectorization in such cases.</li>
</ul>
<h4 id="returns_3">Returns:</h4>
<p>A list or nested structure of Tensors (or None), one for each element in
<code dir="ltr" translate="no">sources</code>. Returned structure is the same as the structure of <code dir="ltr" translate="no">sources</code>.
Note if any gradient is sparse (IndexedSlices), jacobian function
currently makes it dense and returns a Tensor instead. This may change in
the future.</p>
<h4 id="raises_3">Raises:</h4>
<ul>
<li><b><code dir="ltr" translate="no">RuntimeError</code></b>: If called on a non-persistent tape with eager execution
enabled and without enabling experimental_use_pfor.</li>
<li><b><code dir="ltr" translate="no">ValueError</code></b>: If vectorization of jacobian computation fails.</li>
</ul>
<h3 id="reset"><code dir="ltr" translate="no">reset</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/eager/backprop.py#L907-L941" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">reset()
</code></pre>
<p>Clears all information stored in this tape.</p>
<p>Equivalent to exiting and reentering the tape context manager with a new
tape. For example, the two following code blocks are equivalent:</p>
<pre class="prettyprint" dir="ltr" translate="no"><code dir="ltr" translate="no">with tf.GradientTape() as t:
  loss = loss_fn()
with tf.GradientTape() as t:
  loss += other_loss_fn()
t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn


# The following is equivalent to the above
with tf.GradientTape() as t:
  loss = loss_fn()
  t.reset()
  loss += other_loss_fn()
t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn
</code></pre>
<p>This is useful if you don&#39;t want to exit the context manager for the tape,
or can&#39;t because the desired reset point is inside a control flow construct:</p>
<pre class="prettyprint" dir="ltr" translate="no"><code dir="ltr" translate="no">with tf.GradientTape() as t:
  loss = ...
  if loss &gt; k:
    t.reset()
</code></pre>
<h3 id="stop_recording"><code dir="ltr" translate="no">stop_recording</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/eager/backprop.py#L875-L905" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">@contextmanager
stop_recording()
</code></pre>
<p>Temporarily stops recording operations on this tape.</p>
<p>Operations executed while this context manager is active will not be
recorded on the tape. This is useful for reducing the memory used by tracing
all computations.</p>
<h4 id="for_example">For example:</h4>
<pre class="prettyprint" dir="ltr" translate="no"><code dir="ltr" translate="no">  with tf.GradientTape(persistent=True) as t:
    loss = compute_loss(model)
    with t.stop_recording():
      # The gradient computation below is not traced, saving memory.
      grads = t.gradient(loss, model.variables)
</code></pre>
<h4 id="yields">Yields:</h4>
<p>None</p>
<h4 id="raises_4">Raises:</h4>
<ul>
<li><b><code dir="ltr" translate="no">RuntimeError</code></b>: if the tape is not currently recording.</li>
</ul>
<h3 id="watch"><code dir="ltr" translate="no">watch</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/eager/backprop.py#L850-L873" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">watch(
    tensor
)
</code></pre>
<p>Ensures that <code dir="ltr" translate="no">tensor</code> is being traced by this tape.</p>
<h4 id="args_5">Args:</h4>
<ul>
<li><b><code dir="ltr" translate="no">tensor</code></b>: a Tensor or list of Tensors.</li>
</ul>
<h4 id="raises_5">Raises:</h4>
<ul>
<li><b><code dir="ltr" translate="no">ValueError</code></b>: if it encounters something that is not a tensor.</li>
</ul>
<h3 id="watched_variables"><code dir="ltr" translate="no">watched_variables</code></h3>
<p><a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/eager/backprop.py#L943-L947" target="_blank">View source</a></p>
<pre class="prettyprint lang-python" dir="ltr" translate="no"><code dir="ltr" translate="no">watched_variables()
</code></pre>
<p>Returns variables watched by this tape in order of construction.</p>
</div>
<devsite-page-rating hover-rating-star="0" position="footer" selected-rating="0">
</devsite-page-rating>
</article>
</article>

</devsite-content>
</main>
<devsite-footer-promos class="devsite-footer">
</devsite-footer-promos>
<devsite-footer-linkboxes class="devsite-footer">

</devsite-footer-linkboxes>
<devsite-footer-utility class="devsite-footer">
<div class="devsite-footer-utility nocontent">

</div>
</devsite-footer-utility>
</section></section>
<devsite-sitemask></devsite-sitemask>
<devsite-snackbar></devsite-snackbar> <devsite-tooltip></devsite-tooltip>
<devsite-heading-link></devsite-heading-link>
<devsite-analytics>


</devsite-analytics>
 
</body></html>