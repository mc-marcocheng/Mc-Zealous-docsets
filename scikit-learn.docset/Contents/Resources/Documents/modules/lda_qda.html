

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>1.2. Linear and Quadratic Discriminant Analysis &mdash; scikit-learn 0.24.1 documentation</title>
  
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/lda_qda.html" />

  
  <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
<script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script> 
</head>
<body>
<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../index.html">
        <img
          class="sk-brand-img"
          src="../_static/scikit-learn-logo-small.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../install.html">Install</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../user_guide.html">User Guide</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../auto_examples/index.html">Examples</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../getting_started.html">Getting Started</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../tutorial/index.html">Tutorial</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../whats_new/v0.24.html">What's new</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../glossary.html">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../developers/index.html">Development</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../faq.html">FAQ</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../support.html">Support</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../related_projects.html">Related packages</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../roadmap.html">Roadmap</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../about.html">About us</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-learn.org/dev/versions.html">Other Versions and Download</a>
        </li>
        <li class="nav-item dropdown nav-more-item-dropdown">
          <a class="sk-nav-link nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="sk-nav-dropdown-item dropdown-item" href="../getting_started.html">Getting Started</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../tutorial/index.html">Tutorial</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../whats_new/v0.24.html">What's new</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../glossary.html">Glossary</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../developers/index.html">Development</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../faq.html">FAQ</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../support.html">Support</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../related_projects.html">Related packages</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../roadmap.html">Roadmap</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../about.html">About us</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-learn.org/dev/versions.html">Other Versions and Download</a>
          </div>
        </li>
      </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Go" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
    <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
    <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Toggle Menu</label>
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="sk-sidebar-toc-logo">
          <a href="../index.html">
            <img
              class="sk-brand-img"
              src="../_static/scikit-learn-logo-small.png"
              alt="logo"/>
          </a>
        </div>
        <div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
            <a href="linear_model.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1.1. Linear Models">Prev</a><a href="../supervised_learning.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1. Supervised learning">Up</a>
            <a href="kernel_ridge.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1.3. Kernel ridge regression">Next</a>
        </div>
        <div class="alert alert-danger p-1 mb-2" role="alert">
          <p class="text-center mb-0">
          <strong>scikit-learn 0.24.1</strong><br/>
          <a href="http://scikit-learn.org/dev/versions.html">Other versions</a>
          </p>
        </div>
        <div class="alert alert-warning p-1 mb-2" role="alert">
          <p class="text-center mb-0">
            Please <a class="font-weight-bold" href="../about.html#citing-scikit-learn"><string>cite us</string></a> if you use the software.
          </p>
        </div>
            <div class="sk-sidebar-toc">
              <ul>
<li><a class="reference internal" href="#">1.2. Linear and Quadratic Discriminant Analysis</a><ul>
<li><a class="reference internal" href="#dimensionality-reduction-using-linear-discriminant-analysis">1.2.1. Dimensionality reduction using Linear Discriminant Analysis</a></li>
<li><a class="reference internal" href="#mathematical-formulation-of-the-lda-and-qda-classifiers">1.2.2. Mathematical formulation of the LDA and QDA classifiers</a><ul>
<li><a class="reference internal" href="#qda">1.2.2.1. QDA</a></li>
<li><a class="reference internal" href="#lda">1.2.2.2. LDA</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mathematical-formulation-of-lda-dimensionality-reduction">1.2.3. Mathematical formulation of LDA dimensionality reduction</a></li>
<li><a class="reference internal" href="#shrinkage-and-covariance-estimator">1.2.4. Shrinkage and Covariance Estimator</a></li>
<li><a class="reference internal" href="#estimation-algorithms">1.2.5. Estimation algorithms</a></li>
</ul>
</li>
</ul>

            </div>
      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body px-md-3" role="main">
        
  <div class="section" id="linear-and-quadratic-discriminant-analysis">
<span id="lda-qda"></span><h1>1.2. Linear and Quadratic Discriminant Analysis<a class="headerlink" href="#linear-and-quadratic-discriminant-analysis" title="Permalink to this headline">¶</a></h1>
<p>Linear Discriminant Analysis
(<a class="reference internal" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis</span></code></a>) and Quadratic
Discriminant Analysis
(<a class="reference internal" href="generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuadraticDiscriminantAnalysis</span></code></a>) are two classic
classifiers, with, as their names suggest, a linear and a quadratic decision
surface, respectively.</p>
<p>These classifiers are attractive because they have closed-form solutions that
can be easily computed, are inherently multiclass, have proven to work well in
practice, and have no hyperparameters to tune.</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/classification/plot_lda_qda.html"><img alt="ldaqda" src="../_images/sphx_glr_plot_lda_qda_0011.png" style="width: 800.0px; height: 640.0px;" /></a></strong></p><p>The plot shows decision boundaries for Linear Discriminant Analysis and
Quadratic Discriminant Analysis. The bottom row demonstrates that Linear
Discriminant Analysis can only learn linear boundaries, while Quadratic
Discriminant Analysis can learn quadratic boundaries and is therefore more
flexible.</p>
<div class="topic">
<p class="topic-title">Examples:</p>
<p><a class="reference internal" href="../auto_examples/classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py"><span class="std std-ref">Linear and Quadratic Discriminant Analysis with covariance ellipsoid</span></a>: Comparison of LDA and QDA
on synthetic data.</p>
</div>
<div class="section" id="dimensionality-reduction-using-linear-discriminant-analysis">
<h2>1.2.1. Dimensionality reduction using Linear Discriminant Analysis<a class="headerlink" href="#dimensionality-reduction-using-linear-discriminant-analysis" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis</span></code></a> can be used to
perform supervised dimensionality reduction, by projecting the input data to a
linear subspace consisting of the directions which maximize the separation
between classes (in a precise sense discussed in the mathematics section
below). The dimension of the output is necessarily less than the number of
classes, so this is in general a rather strong dimensionality reduction, and
only makes sense in a multiclass setting.</p>
<p>This is implemented in the <code class="docutils literal notranslate"><span class="pre">transform</span></code> method. The desired dimensionality can
be set using the <code class="docutils literal notranslate"><span class="pre">n_components</span></code> parameter. This parameter has no influence
on the <code class="docutils literal notranslate"><span class="pre">fit</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code> methods.</p>
<div class="topic">
<p class="topic-title">Examples:</p>
<p><a class="reference internal" href="../auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"><span class="std std-ref">Comparison of LDA and PCA 2D projection of Iris dataset</span></a>: Comparison of LDA and PCA
for dimensionality reduction of the Iris dataset</p>
</div>
</div>
<div class="section" id="mathematical-formulation-of-the-lda-and-qda-classifiers">
<span id="lda-qda-math"></span><h2>1.2.2. Mathematical formulation of the LDA and QDA classifiers<a class="headerlink" href="#mathematical-formulation-of-the-lda-and-qda-classifiers" title="Permalink to this headline">¶</a></h2>
<p>Both LDA and QDA can be derived from simple probabilistic models which model
the class conditional distribution of the data <img class="math" src="../_images/math/6607784e1df02874dd65af73c6179c53da072375.svg" alt="P(X|y=k)"/> for each class
<img class="math" src="../_images/math/9630132210b904754c9ab272b61cb527d12263ca.svg" alt="k"/>. Predictions can then be obtained by using Bayes’ rule, for each
training sample <img class="math" src="../_images/math/6c59e7ddaca7055f6055a471d2a5eb3e86bd8520.svg" alt="x \in \mathcal{R}^d"/>:</p>
<div class="math">
<p><img src="../_images/math/65183f5c58b90cc8b2645a6e1de927a343f173b4.svg" alt="P(y=k | x) = \frac{P(x | y=k) P(y=k)}{P(x)} = \frac{P(x | y=k) P(y = k)}{ \sum_{l} P(x | y=l) \cdot P(y=l)}"/></p>
</div><p>and we select the class <img class="math" src="../_images/math/9630132210b904754c9ab272b61cb527d12263ca.svg" alt="k"/> which maximizes this posterior probability.</p>
<p>More specifically, for linear and quadratic discriminant analysis,
<img class="math" src="../_images/math/8c8d8cf2c032a94ffc823b939cce6347430a26be.svg" alt="P(x|y)"/> is modeled as a multivariate Gaussian distribution with
density:</p>
<div class="math">
<p><img src="../_images/math/0405a11d47f40644c4db8ba144177e4eb34d30b9.svg" alt="P(x | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k)\right)"/></p>
</div><p>where <img class="math" src="../_images/math/badad346f6fbe2e237af99bfbd9a93a4da53a3da.svg" alt="d"/> is the number of features.</p>
<div class="section" id="qda">
<h3>1.2.2.1. QDA<a class="headerlink" href="#qda" title="Permalink to this headline">¶</a></h3>
<p>According to the model above, the log of the posterior is:</p>
<div class="math">
<p><img src="../_images/math/3d2cecd23c7eb04203a5a218dbb29a26481102cc.svg" alt="\log P(y=k | x) &amp;= \log P(x | y=k) + \log P(y = k) + Cst \\
&amp;= -\frac{1}{2} \log |\Sigma_k| -\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k) + \log P(y = k) + Cst,"/></p>
</div><p>where the constant term <img class="math" src="../_images/math/0d903f8878d64223561899e022be182fb6106d33.svg" alt="Cst"/> corresponds to the denominator
<img class="math" src="../_images/math/f01cb75ce76498bd7c3e6f4505d157bc0315ce1d.svg" alt="P(x)"/>, in addition to other constant terms from the Gaussian. The
predicted class is the one that maximises this log-posterior.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Relation with Gaussian Naive Bayes</strong></p>
<p>If in the QDA model one assumes that the covariance matrices are diagonal,
then the inputs are assumed to be conditionally independent in each class,
and the resulting classifier is equivalent to the Gaussian Naive Bayes
classifier <a class="reference internal" href="generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" title="sklearn.naive_bayes.GaussianNB"><code class="xref py py-class docutils literal notranslate"><span class="pre">naive_bayes.GaussianNB</span></code></a>.</p>
</div>
</div>
<div class="section" id="lda">
<h3>1.2.2.2. LDA<a class="headerlink" href="#lda" title="Permalink to this headline">¶</a></h3>
<p>LDA is a special case of QDA, where the Gaussians for each class are assumed
to share the same covariance matrix: <img class="math" src="../_images/math/755b37d638dfa6154f2e6991d1b1efd14acd748a.svg" alt="\Sigma_k = \Sigma"/> for all
<img class="math" src="../_images/math/9630132210b904754c9ab272b61cb527d12263ca.svg" alt="k"/>. This reduces the log posterior to:</p>
<div class="math">
<p><img src="../_images/math/a287c9235e5c24248a5512a3fd1d93d8d3bc8782.svg" alt="\log P(y=k | x) = -\frac{1}{2} (x-\mu_k)^t \Sigma^{-1} (x-\mu_k) + \log P(y = k) + Cst."/></p>
</div><p>The term <img class="math" src="../_images/math/1c04c7151f6873f531abe984408c40f188239cfc.svg" alt="(x-\mu_k)^t \Sigma^{-1} (x-\mu_k)"/> corresponds to the
<a class="reference external" href="https://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis Distance</a>
between the sample <img class="math" src="../_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.svg" alt="x"/> and the mean <img class="math" src="../_images/math/5e53f03a70bc20cfe803bc4ecee6d2e396a89aed.svg" alt="\mu_k"/>. The Mahalanobis
distance tells how close <img class="math" src="../_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.svg" alt="x"/> is from <img class="math" src="../_images/math/5e53f03a70bc20cfe803bc4ecee6d2e396a89aed.svg" alt="\mu_k"/>, while also
accounting for the variance of each feature. We can thus interpret LDA as
assigning <img class="math" src="../_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.svg" alt="x"/> to the class whose mean is the closest in terms of
Mahalanobis distance, while also accounting for the class prior
probabilities.</p>
<p>The log-posterior of LDA can also be written <a class="footnote-reference brackets" href="#id7" id="id1">3</a> as:</p>
<div class="math">
<p><img src="../_images/math/5961fd0987a9b74d255b9319adf6ed8d6740deee.svg" alt="\log P(y=k | x) = \omega_k^t x + \omega_{k0} + Cst."/></p>
</div><p>where <img class="math" src="../_images/math/e42273e1bcee30a04ec50872ad91da261d43ea41.svg" alt="\omega_k = \Sigma^{-1} \mu_k"/> and <img class="math" src="../_images/math/6b47e11fd0fd602c9a5b5ba18abb1e5d02a5d399.svg" alt="\omega_{k0} =
-\frac{1}{2} \mu_k^t\Sigma^{-1}\mu_k + \log P (y = k)"/>. These quantities
correspond to the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> attributes, respectively.</p>
<p>From the above formula, it is clear that LDA has a linear decision surface.
In the case of QDA, there are no assumptions on the covariance matrices
<img class="math" src="../_images/math/33ef9c378d2f0151cddda7cbc9bbe205b18b826e.svg" alt="\Sigma_k"/> of the Gaussians, leading to quadratic decision surfaces.
See <a class="footnote-reference brackets" href="#id5" id="id2">1</a> for more details.</p>
</div>
</div>
<div class="section" id="mathematical-formulation-of-lda-dimensionality-reduction">
<h2>1.2.3. Mathematical formulation of LDA dimensionality reduction<a class="headerlink" href="#mathematical-formulation-of-lda-dimensionality-reduction" title="Permalink to this headline">¶</a></h2>
<p>First note that the K means <img class="math" src="../_images/math/5e53f03a70bc20cfe803bc4ecee6d2e396a89aed.svg" alt="\mu_k"/> are vectors in
<img class="math" src="../_images/math/103c32aebc2b3441656f73c2bd6dd0fcc614ca12.svg" alt="\mathcal{R}^d"/>, and they lie in an affine subspace <img class="math" src="../_images/math/cb5de54f699cf4b3c7c1a3e87313d11d536c0d88.svg" alt="H"/> of
dimension at least <img class="math" src="../_images/math/b099c91cfee5d95cfca0a01fe608dc71911b7f73.svg" alt="K - 1"/> (2 points lie on a line, 3 points lie on a
plane, etc).</p>
<p>As mentioned above, we can interpret LDA as assigning <img class="math" src="../_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.svg" alt="x"/> to the class
whose mean <img class="math" src="../_images/math/5e53f03a70bc20cfe803bc4ecee6d2e396a89aed.svg" alt="\mu_k"/> is the closest in terms of Mahalanobis distance,
while also accounting for the class prior probabilities. Alternatively, LDA
is equivalent to first <em>sphering</em> the data so that the covariance matrix is
the identity, and then assigning <img class="math" src="../_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.svg" alt="x"/> to the closest mean in terms of
Euclidean distance (still accounting for the class priors).</p>
<p>Computing Euclidean distances in this d-dimensional space is equivalent to
first projecting the data points into <img class="math" src="../_images/math/cb5de54f699cf4b3c7c1a3e87313d11d536c0d88.svg" alt="H"/>, and computing the distances
there (since the other dimensions will contribute equally to each class in
terms of distance). In other words, if <img class="math" src="../_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.svg" alt="x"/> is closest to <img class="math" src="../_images/math/5e53f03a70bc20cfe803bc4ecee6d2e396a89aed.svg" alt="\mu_k"/>
in the original space, it will also be the case in <img class="math" src="../_images/math/cb5de54f699cf4b3c7c1a3e87313d11d536c0d88.svg" alt="H"/>.
This shows that, implicit in the LDA
classifier, there is a dimensionality reduction by linear projection onto a
<img class="math" src="../_images/math/ba4d9d4daad4aa607a1b9645de4dff5d53aa7986.svg" alt="K-1"/> dimensional space.</p>
<p>We can reduce the dimension even more, to a chosen <img class="math" src="../_images/math/19eef1966f7c545af3ac8c0fa486974d873e3c65.svg" alt="L"/>, by projecting
onto the linear subspace <img class="math" src="../_images/math/2a6e019154e93c8492dcd0d3e69ae204c1dbf835.svg" alt="H_L"/> which maximizes the variance of the
<img class="math" src="../_images/math/5005c125c391c73842f27bcdfa149299f6eb47d9.svg" alt="\mu^*_k"/> after projection (in effect, we are doing a form of PCA for the
transformed class means <img class="math" src="../_images/math/5005c125c391c73842f27bcdfa149299f6eb47d9.svg" alt="\mu^*_k"/>). This <img class="math" src="../_images/math/19eef1966f7c545af3ac8c0fa486974d873e3c65.svg" alt="L"/> corresponds to the
<code class="docutils literal notranslate"><span class="pre">n_components</span></code> parameter used in the
<a class="reference internal" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform"><code class="xref py py-func docutils literal notranslate"><span class="pre">transform</span></code></a> method. See
<a class="footnote-reference brackets" href="#id5" id="id3">1</a> for more details.</p>
</div>
<div class="section" id="shrinkage-and-covariance-estimator">
<h2>1.2.4. Shrinkage and Covariance Estimator<a class="headerlink" href="#shrinkage-and-covariance-estimator" title="Permalink to this headline">¶</a></h2>
<p>Shrinkage is a form of regularization used to improve the estimation of
covariance matrices in situations where the number of training samples is
small compared to the number of features.
In this scenario, the empirical sample covariance is a poor
estimator, and shrinkage helps improving the generalization performance of
the classifier.
Shrinkage LDA can be used by setting the <code class="docutils literal notranslate"><span class="pre">shrinkage</span></code> parameter of
the <a class="reference internal" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis</span></code></a> class to ‘auto’.
This automatically determines the optimal shrinkage parameter in an analytic
way following the lemma introduced by Ledoit and Wolf <a class="footnote-reference brackets" href="#id6" id="id4">2</a>. Note that
currently shrinkage only works when setting the <code class="docutils literal notranslate"><span class="pre">solver</span></code> parameter to ‘lsqr’
or ‘eigen’.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">shrinkage</span></code> parameter can also be manually set between 0 and 1. In
particular, a value of 0 corresponds to no shrinkage (which means the empirical
covariance matrix will be used) and a value of 1 corresponds to complete
shrinkage (which means that the diagonal matrix of variances will be used as
an estimate for the covariance matrix). Setting this parameter to a value
between these two extrema will estimate a shrunk version of the covariance
matrix.</p>
<p>The shrinked Ledoit and Wolf estimator of covariance may not always be the
best choice. For example if the distribution of the data
is normally distributed, the
Oracle Shrinkage Approximating estimator <a class="reference internal" href="generated/sklearn.covariance.OAS.html#sklearn.covariance.OAS" title="sklearn.covariance.OAS"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.covariance.OAS</span></code></a>
yields a smaller Mean Squared Error than the one given by Ledoit and Wolf’s
formula used with shrinkage=”auto”. In LDA, the data are assumed to be gaussian
conditionally to the class. If these assumptions hold, using LDA with
the OAS estimator of covariance will yield a better classification
accuracy than if Ledoit and Wolf or the empirical covariance estimator is used.</p>
<p>The covariance estimator can be chosen using with the <code class="docutils literal notranslate"><span class="pre">covariance_estimator</span></code>
parameter of the <a class="reference internal" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">discriminant_analysis.LinearDiscriminantAnalysis</span></code></a>
class. A covariance estimator should have a <a class="reference internal" href="../glossary.html#term-fit"><span class="xref std std-term">fit</span></a> method and a
<code class="docutils literal notranslate"><span class="pre">covariance_</span></code> attribute like all covariance estimators in the
<a class="reference internal" href="classes.html#module-sklearn.covariance" title="sklearn.covariance"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.covariance</span></code></a> module.</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/classification/plot_lda.html"><img alt="shrinkage" src="../_images/sphx_glr_plot_lda_0011.png" style="width: 480.0px; height: 360.0px;" /></a></strong></p><div class="topic">
<p class="topic-title">Examples:</p>
<p><a class="reference internal" href="../auto_examples/classification/plot_lda.html#sphx-glr-auto-examples-classification-plot-lda-py"><span class="std std-ref">Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification</span></a>: Comparison of LDA classifiers
with Empirical, Ledoit Wolf and OAS covariance estimator.</p>
</div>
</div>
<div class="section" id="estimation-algorithms">
<h2>1.2.5. Estimation algorithms<a class="headerlink" href="#estimation-algorithms" title="Permalink to this headline">¶</a></h2>
<p>Using LDA and QDA requires computing the log-posterior which depends on the
class priors <img class="math" src="../_images/math/0be3810dc42e174694c79725feb6121ea64f4d57.svg" alt="P(y=k)"/>, the class means <img class="math" src="../_images/math/5e53f03a70bc20cfe803bc4ecee6d2e396a89aed.svg" alt="\mu_k"/>, and the
covariance matrices.</p>
<p>The ‘svd’ solver is the default solver used for
<a class="reference internal" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis</span></code></a>, and it is
the only available solver for
<a class="reference internal" href="generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuadraticDiscriminantAnalysis</span></code></a>.
It can perform both classification and transform (for LDA).
As it does not rely on the calculation of the covariance matrix, the ‘svd’
solver may be preferable in situations where the number of features is large.
The ‘svd’ solver cannot be used with shrinkage.
For QDA, the use of the SVD solver relies on the fact that the covariance
matrix <img class="math" src="../_images/math/33ef9c378d2f0151cddda7cbc9bbe205b18b826e.svg" alt="\Sigma_k"/> is, by definition, equal to <img class="math" src="../_images/math/c91b13a5e3b9687b4cde6c97052d35a25fb092ad.svg" alt="\frac{1}{n - 1}
X_k^tX_k = V S^2 V^t"/> where <img class="math" src="../_images/math/e4762cec46619bf7781cae62216214f909395368.svg" alt="V"/> comes from the SVD of the (centered)
matrix: <img class="math" src="../_images/math/9dfca4b30c873f4287d3bdc0326f9c6069971470.svg" alt="X_k = U S V^t"/>. It turns out that we can compute the
log-posterior above without having to explictly compute <img class="math" src="../_images/math/6edc5c119344e25a06e6ac4cb56f2d5e2f09a2f1.svg" alt="\Sigma"/>:
computing <img class="math" src="../_images/math/b988975be41fd13b4d091c10202ba19374643586.svg" alt="S"/> and <img class="math" src="../_images/math/e4762cec46619bf7781cae62216214f909395368.svg" alt="V"/> via the SVD of <img class="math" src="../_images/math/ed38fa24f1c94891bd312012aab3f6673be3eb83.svg" alt="X"/> is enough. For
LDA, two SVDs are computed: the SVD of the centered input matrix <img class="math" src="../_images/math/ed38fa24f1c94891bd312012aab3f6673be3eb83.svg" alt="X"/>
and the SVD of the class-wise mean vectors.</p>
<p>The ‘lsqr’ solver is an efficient algorithm that only works for
classification. It needs to explicitly compute the covariance matrix
<img class="math" src="../_images/math/6edc5c119344e25a06e6ac4cb56f2d5e2f09a2f1.svg" alt="\Sigma"/>, and supports shrinkage and custom covariance estimators.
This solver computes the coefficients
<img class="math" src="../_images/math/3668d700ceaa5d4e60f287ca0e96fba6a5fa4a5c.svg" alt="\omega_k = \Sigma^{-1}\mu_k"/> by solving for <img class="math" src="../_images/math/7a9bc2bfb40f2ec3d602e2a0b4685018fef5783b.svg" alt="\Sigma \omega =
\mu_k"/>, thus avoiding the explicit computation of the inverse
<img class="math" src="../_images/math/4bbab65214f47e0c5164d88d22cef380ee5b3e58.svg" alt="\Sigma^{-1}"/>.</p>
<p>The ‘eigen’ solver is based on the optimization of the between class scatter to
within class scatter ratio. It can be used for both classification and
transform, and it supports shrinkage. However, the ‘eigen’ solver needs to
compute the covariance matrix, so it might not be suitable for situations with
a high number of features.</p>
<div class="topic">
<p class="topic-title">References:</p>
<dl class="footnote brackets">
<dt class="label" id="id5"><span class="brackets">1</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id3">2</a>)</span></dt>
<dd><p>“The Elements of Statistical Learning”, Hastie T., Tibshirani R.,
Friedman J., Section 4.3, p.106-119, 2008.</p>
</dd>
<dt class="label" id="id6"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix.
The Journal of Portfolio Management 30(4), 110-119, 2004.</p>
</dd>
<dt class="label" id="id7"><span class="brackets"><a class="fn-backref" href="#id1">3</a></span></dt>
<dd><p>R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification
(Second Edition), section 2.6.2.</p>
</dd>
</dl>
</div>
</div>
</div>


      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2007 - 2020, scikit-learn developers (BSD License).
          <a href="../_sources/modules/lda_qda.rst.txt" rel="nofollow">Show this page source</a>
      </footer>
    </div>
  </div>
</div>
<script src="../_static/js/vendor/bootstrap.min.js"></script>

<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    
</body>
</html>