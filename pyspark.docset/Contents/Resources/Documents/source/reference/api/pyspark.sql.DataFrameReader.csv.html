
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<title>pyspark.sql.DataFrameReader.csv — PySpark master documentation</title>
<link href="../../../_static/nature.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/pyspark.css" rel="stylesheet" type="text/css"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/language_data.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../../../_static/pyspark.js"></script>
<link href="../../../search.html" rel="search" title="Search"/>
</head><body>
<div aria-label="related navigation" class="related" role="navigation">
<h3>Navigation</h3>
<ul>
<li class="nav-item nav-item-0"><a href="../../../index.html">PySpark master documentation</a> »</li>
</ul>
</div>
<div class="document">
<div class="documentwrapper">
<div class="body" role="main">
<div class="section" id="pyspark-sql-dataframereader-csv">
<h1>pyspark.sql.DataFrameReader.csv<a class="headerlink" href="#pyspark-sql-dataframereader-csv" title="Permalink to this headline">¶</a></h1>
<dl class="py method">
<dt id="pyspark.sql.DataFrameReader.csv"><a name="//apple_ref/cpp/Method/pyspark.sql.DataFrameReader.csv"></a>
<code class="sig-prename descclassname">DataFrameReader.</code><code class="sig-name descname">csv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">path</span></em>, <em class="sig-param"><span class="n">schema</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sep</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">encoding</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">quote</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">escape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">comment</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">header</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inferSchema</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ignoreLeadingWhiteSpace</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ignoreTrailingWhiteSpace</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">nullValue</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">nanValue</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">positiveInf</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">negativeInf</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dateFormat</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">timestampFormat</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">maxColumns</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">maxCharsPerColumn</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">maxMalformedLogPerPartition</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">mode</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">columnNameOfCorruptRecord</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">multiLine</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">charToEscapeQuoteEscaping</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">samplingRatio</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">enforceSchema</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">emptyValue</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">locale</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">lineSep</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pathGlobFilter</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">recursiveFileLookup</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/pyspark/sql/readwriter.html#DataFrameReader.csv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.csv" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a CSV file and returns the result as a  <a class="reference internal" href="pyspark.sql.DataFrame.html#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<p>This function will go through the input once to determine the input schema if
<code class="docutils literal notranslate"><span class="pre">inferSchema</span></code> is enabled. To avoid going through the entire data once, disable
<code class="docutils literal notranslate"><span class="pre">inferSchema</span></code> option or specify the schema explicitly using <code class="docutils literal notranslate"><span class="pre">schema</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – string, or list of strings, for input path(s),
or RDD of Strings storing CSV rows.</p></li>
<li><p><strong>schema</strong> – an optional <a class="reference internal" href="pyspark.sql.types.StructType.html#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> for the input schema
or a DDL-formatted string (For example <code class="docutils literal notranslate"><span class="pre">col0</span> <span class="pre">INT,</span> <span class="pre">col1</span> <span class="pre">DOUBLE</span></code>).</p></li>
<li><p><strong>sep</strong> – sets a separator (one or more characters) for each field and value. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">,</span></code>.</p></li>
<li><p><strong>encoding</strong> – decodes the CSV files by the given encoding type. If None is set,
it uses the default value, <code class="docutils literal notranslate"><span class="pre">UTF-8</span></code>.</p></li>
<li><p><strong>quote</strong> – sets a single character used for escaping quoted values where the
separator can be part of the value. If None is set, it uses the default
value, <code class="docutils literal notranslate"><span class="pre">"</span></code>. If you would like to turn off quotations, you need to set an
empty string.</p></li>
<li><p><strong>escape</strong> – sets a single character used for escaping quotes inside an already
quoted value. If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">\</span></code>.</p></li>
<li><p><strong>comment</strong> – sets a single character used for skipping lines beginning with this
character. By default (None), it is disabled.</p></li>
<li><p><strong>header</strong> – uses the first line as names of columns. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>inferSchema</strong> – infers the input schema automatically from data. It requires one extra
pass over the data. If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>enforceSchema</strong> – If it is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the specified or inferred schema will be
forcibly applied to datasource files, and headers in CSV files will be
ignored. If the option is set to <code class="docutils literal notranslate"><span class="pre">false</span></code>, the schema will be
validated against all headers in CSV files or the first header in RDD
if the <code class="docutils literal notranslate"><span class="pre">header</span></code> option is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>. Field names in the schema
and column names in CSV headers are checked by their positions
taking into account <code class="docutils literal notranslate"><span class="pre">spark.sql.caseSensitive</span></code>. If None is set,
<code class="docutils literal notranslate"><span class="pre">true</span></code> is used by default. Though the default value is <code class="docutils literal notranslate"><span class="pre">true</span></code>,
it is recommended to disable the <code class="docutils literal notranslate"><span class="pre">enforceSchema</span></code> option
to avoid incorrect results.</p></li>
<li><p><strong>ignoreLeadingWhiteSpace</strong> – A flag indicating whether or not leading whitespaces from
values being read should be skipped. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>ignoreTrailingWhiteSpace</strong> – A flag indicating whether or not trailing whitespaces from
values being read should be skipped. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>nullValue</strong> – sets the string representation of a null value. If None is set, it uses
the default value, empty string. Since 2.0.1, this <code class="docutils literal notranslate"><span class="pre">nullValue</span></code> param
applies to all supported types including the string type.</p></li>
<li><p><strong>nanValue</strong> – sets the string representation of a non-number value. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">NaN</span></code>.</p></li>
<li><p><strong>positiveInf</strong> – sets the string representation of a positive infinity value. If None
is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">Inf</span></code>.</p></li>
<li><p><strong>negativeInf</strong> – sets the string representation of a negative infinity value. If None
is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">Inf</span></code>.</p></li>
<li><p><strong>dateFormat</strong> – sets the string that indicates a date format. Custom date formats
follow the formats at <a href="#id1"><span class="problematic" id="id2">`datetime pattern`_</span></a>.
This applies to date type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd</span></code>.</p></li>
<li><p><strong>timestampFormat</strong> – sets the string that indicates a timestamp format.
Custom date formats follow the formats at <a href="#id3"><span class="problematic" id="id4">`datetime pattern`_</span></a>.
This applies to timestamp type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]</span></code>.</p></li>
<li><p><strong>maxColumns</strong> – defines a hard limit of how many columns a record can have. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">20480</span></code>.</p></li>
<li><p><strong>maxCharsPerColumn</strong> – defines the maximum number of characters allowed for any given
value being read. If None is set, it uses the default value,
<code class="docutils literal notranslate"><span class="pre">-1</span></code> meaning unlimited length.</p></li>
<li><p><strong>maxMalformedLogPerPartition</strong> – this parameter is no longer used since Spark 2.2.0.
If specified, it is ignored.</p></li>
<li><p><strong>mode</strong> – <dl class="simple">
<dt>allows a mode for dealing with corrupt records during parsing. If None is</dt><dd><p>set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code>. Note that Spark tries to
parse only required columns in CSV under column pruning. Therefore, corrupt
records can be different based on required set of fields. This behavior can
be controlled by <code class="docutils literal notranslate"><span class="pre">spark.sql.csv.parser.columnPruning.enabled</span></code>
(enabled by default).</p>
</dd>
</dl>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code>: when it meets a corrupted record, puts the malformed string into a field configured by <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code>, and sets malformed fields to <code class="docutils literal notranslate"><span class="pre">null</span></code>. To keep corrupt records, an user can set a string type field named <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code> in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. A record with less/more tokens than schema is not a corrupted record to CSV. When it meets a record having fewer tokens than the length of the schema, sets <code class="docutils literal notranslate"><span class="pre">null</span></code> to extra fields. When the record has more tokens than the length of the schema, it drops extra tokens.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DROPMALFORMED</span></code>: ignores the whole corrupted records.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FAILFAST</span></code>: throws an exception when it meets corrupted records.</p></li>
</ul>
</p></li>
<li><p><strong>columnNameOfCorruptRecord</strong> – allows renaming the new field having malformed string
created by <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code> mode. This overrides
<code class="docutils literal notranslate"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></code>. If None is set,
it uses the value specified in
<code class="docutils literal notranslate"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></code>.</p></li>
<li><p><strong>multiLine</strong> – parse records, which may span multiple lines. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>charToEscapeQuoteEscaping</strong> – sets a single character used for escaping the escape for
the quote character. If None is set, the default value is
escape character when escape and quote characters are
different, <code class="docutils literal notranslate"><span class="pre">\0</span></code> otherwise.</p></li>
<li><p><strong>samplingRatio</strong> – defines fraction of rows used for schema inferring.
If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><strong>emptyValue</strong> – sets the string representation of an empty value. If None is set, it uses
the default value, empty string.</p></li>
<li><p><strong>locale</strong> – sets a locale as language tag in IETF BCP 47 format. If None is set,
it uses the default value, <code class="docutils literal notranslate"><span class="pre">en-US</span></code>. For instance, <code class="docutils literal notranslate"><span class="pre">locale</span></code> is used while
parsing dates and timestamps.</p></li>
<li><p><strong>lineSep</strong> – defines the line separator that should be used for parsing. If None is
set, it covers all <code class="docutils literal notranslate"><span class="pre">\\r</span></code>, <code class="docutils literal notranslate"><span class="pre">\\r\\n</span></code> and <code class="docutils literal notranslate"><span class="pre">\\n</span></code>.
Maximum length is 1 character.</p></li>
<li><p><strong>pathGlobFilter</strong> – an optional glob pattern to only include files with paths matching
the pattern. The syntax follows <cite>org.apache.hadoop.fs.GlobFilter</cite>.
It does not change the behavior of <a href="#id5"><span class="problematic" id="id6">`partition discovery`_</span></a>.</p></li>
<li><p><strong>recursiveFileLookup</strong> – recursively scan a directory for files. Using this option
disables <a href="#id7"><span class="problematic" id="id8">`partition discovery`_</span></a>.</p></li>
</ul>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s1">'python/test_support/sql/ages.csv'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[('_c0', 'string'), ('_c1', 'string')]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s1">'python/test_support/sql/ages.csv'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[('_c0', 'string'), ('_c1', 'string')]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>
</div>
</div>
</div>
<div class="clearer"></div>
</div>
<div aria-label="related navigation" class="related" role="navigation">
<h3>Navigation</h3>
<ul>
<li class="nav-item nav-item-0"><a href="../../../index.html">PySpark master documentation</a> »</li>
</ul>
</div>
<div class="footer" role="contentinfo">
        © Copyright .
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.
    </div>
</body>
</html>