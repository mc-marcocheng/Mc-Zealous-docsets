
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<title>pyspark.SparkContext.sequenceFile — PySpark master documentation</title>
<link href="../../../_static/nature.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/pyspark.css" rel="stylesheet" type="text/css"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/language_data.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../../../_static/pyspark.js"></script>
<link href="../../../search.html" rel="search" title="Search"/>
</head><body>
<div aria-label="related navigation" class="related" role="navigation">
<h3>Navigation</h3>
<ul>
<li class="nav-item nav-item-0"><a href="../../../index.html">PySpark master documentation</a> »</li>
</ul>
</div>
<div class="document">
<div class="documentwrapper">
<div class="body" role="main">
<div class="section" id="pyspark-sparkcontext-sequencefile">
<h1>pyspark.SparkContext.sequenceFile<a class="headerlink" href="#pyspark-sparkcontext-sequencefile" title="Permalink to this headline">¶</a></h1>
<dl class="py method">
<dt id="pyspark.SparkContext.sequenceFile"><a name="//apple_ref/cpp/Method/pyspark.SparkContext.sequenceFile"></a>
<code class="sig-prename descclassname">SparkContext.</code><code class="sig-name descname">sequenceFile</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">path</span></em>, <em class="sig-param"><span class="n">keyClass</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">valueClass</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keyConverter</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">valueConverter</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">minSplits</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batchSize</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/pyspark/context.html#SparkContext.sequenceFile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.SparkContext.sequenceFile" title="Permalink to this definition">¶</a></dt>
<dd><p>Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,
a local file system (available on all nodes), or any Hadoop-supported file system URI.
The mechanism is as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>A Java RDD is created from the SequenceFile or other InputFormat, and the key
and value Writable classes</p></li>
<li><p>Serialization is attempted via Pyrolite pickling</p></li>
<li><p>If this fails, the fallback is to call ‘toString’ on each key and value</p></li>
<li><p><a class="reference internal" href="../../../pyspark.html#pyspark.PickleSerializer" title="pyspark.PickleSerializer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PickleSerializer</span></code></a> is used to deserialize pickled objects on the Python side</p></li>
</ol>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – path to sequncefile</p></li>
<li><p><strong>keyClass</strong> – fully qualified classname of key Writable class
(e.g. “org.apache.hadoop.io.Text”)</p></li>
<li><p><strong>valueClass</strong> – fully qualified classname of value Writable class
(e.g. “org.apache.hadoop.io.LongWritable”)</p></li>
<li><p><strong>keyConverter</strong> – </p></li>
<li><p><strong>valueConverter</strong> – </p></li>
<li><p><strong>minSplits</strong> – minimum splits in dataset
(default min(2, sc.defaultParallelism))</p></li>
<li><p><strong>batchSize</strong> – The number of Python objects represented as a single
Java object. (default 0, choose batchSize automatically)</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
</div>
</div>
<div class="clearer"></div>
</div>
<div aria-label="related navigation" class="related" role="navigation">
<h3>Navigation</h3>
<ul>
<li class="nav-item nav-item-0"><a href="../../../index.html">PySpark master documentation</a> »</li>
</ul>
</div>
<div class="footer" role="contentinfo">
        © Copyright .
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.
    </div>
</body>
</html>