<html><head><script type="text/javascript" async="" src="http://www.google-analytics.com/ga.js"></script><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-33108845-1']);
  _gaq.push(['_setDomainName', 'opencv.org']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>


  
<!-- Mirrored from docs.opencv.org/3.0-last-rst/modules/xfeatures2d/doc/nonfree_features.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 23 Dec 2015 06:59:19 GMT -->

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    
    <title>Non-free 2D Features Algorithms</title>
    
    <link rel="stylesheet" href="../../../_static/default.css" type="text/css">
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css">
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '3.0.0-dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/insertIframe.js"></script>
    <link rel="top" title="OpenCV 3.0.0-dev documentation" href="../../../index.html">
    <link rel="up" title="xfeatures2d. Extra 2D Features Framework" href="xfeatures2d.html">
    <link rel="next" title="ximgproc. Extended Image Processing" href="../../ximgproc/doc/ximgproc.html">
    <link rel="prev" title="Experimental 2D Features Algorithms" href="extra_features.html">
    <link href="../../../../../fonts.googleapis.com/css8a7c.css?family=Open+Sans:300,400,700" rel="stylesheet" type="text/css">
    <style type="text/css">
      table.right { float: right; margin-left: 20px; }
      table.right td { border: 1px solid #ccc; }
    </style>
    <script type="text/javascript">
      // intelligent scrolling of the sidebar content
      $(window).scroll(function() {
        var sb = $('.sphinxsidebarwrapper');
        var win = $(window);
        var sbh = sb.height();
        var offset = $('.sphinxsidebar').position()['top'];
        var wintop = win.scrollTop();
        var winbot = wintop + win.innerHeight();
        var curtop = sb.position()['top'];
        var curbot = curtop + sbh;
        // does sidebar fit in window?
        if (sbh < win.innerHeight()) {
          // yes: easy case -- always keep at the top
          sb.css('top', $u.min([$u.max([0, wintop - offset - 10]),
                                $(document).height() - sbh - 200]));
        } else {
          // no: only scroll if top/bottom edge of sidebar is at
          // top/bottom edge of window
          if (curtop > wintop && curbot > winbot) {
            sb.css('top', $u.max([wintop - offset - 10, 0]));
          } else if (curtop < wintop && curbot < winbot) {
            sb.css('top', $u.min([winbot - sbh - offset - 20,
                                  $(document).height() - sbh - 200]));
          }
        }
      });
    </script>

  </head>
    <body><div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index" accesskey="I">index</a></li>
        <li class="right">
          <a href="../../ximgproc/doc/ximgproc.html" title="ximgproc. Extended Image Processing" accesskey="N">next</a> |</li>
        <li class="right">
          <a href="extra_features.html" title="Experimental 2D Features Algorithms" accesskey="P">previous</a> |</li>
        <li><a href="../../../index.html">OpenCV 3.0.0-dev documentation</a> »</li>
          <li><a href="../../refman.html">OpenCV API Reference</a> »</li>
          <li><a href="xfeatures2d.html" accesskey="U">xfeatures2d. Extra 2D Features Framework</a> »</li> 
      </ul>
    </div>  
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/opencv-logo2.png" alt="Logo">
            </a></p>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="http://docs.opencv.org/3.0-last-rst/search.html" method="get">
      <input type="text" name="q">
      <input type="submit" value="Go">
      <input type="hidden" name="check_keywords" value="yes">
      <input type="hidden" name="area" value="default">
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
  <h3><a href="../../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Non-free 2D Features Algorithms</a><ul>
<li><a class="reference internal" href="#sift">SIFT</a></li>
<li><a class="reference internal" href="#sift-sift">SIFT::SIFT</a></li>
<li><a class="reference internal" href="#sift-operator">SIFT::operator ()</a></li>
<li><a class="reference internal" href="#surf">SURF</a></li>
<li><a class="reference internal" href="#surf-surf">SURF::SURF</a></li>
<li><a class="reference internal" href="#surf-operator">SURF::operator()</a></li>
<li><a class="reference internal" href="#cuda-surf-cuda">cuda::SURF_CUDA</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="extra_features.html" title="previous chapter">Experimental 2D Features Algorithms</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../../ximgproc/doc/ximgproc.html" title="next chapter">ximgproc. Extended Image Processing</a></p>
        </div>
      </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="non-free-2d-features-algorithms">
<h1>Non-free 2D Features Algorithms<a class="headerlink" href="#non-free-2d-features-algorithms" title="Permalink to this headline">¶</a></h1>
<p>This section describes two popular algorithms for 2d feature detection, SIFT and SURF, that are known to be patented. Use them at your own risk.</p>
<div class="section" id="sift">
<h2>SIFT<a class="headerlink" href="#sift" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="SIFT : public Feature2D">
<em class="property">class </em><tt class="descname">SIFT</tt> : <em class="property">public</em> <tt class="descname">Feature2D</tt><a class="headerlink" href="#SIFT : public Feature2D" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (SIFT) algorithm by D. Lowe <a class="reference internal" href="#lowe04" id="id1">[Lowe04]</a>.</p>
<table class="docutils citation" frame="void" id="lowe04" rules="none">
<colgroup><col class="label"><col></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[Lowe04]</a></td><td>Lowe, D. G., “Distinctive Image Features from Scale-Invariant Keypoints”, International Journal of Computer Vision, 60, 2, pp. 91-110, 2004.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="sift-sift">
<h2>SIFT::SIFT<a class="headerlink" href="#sift-sift" title="Permalink to this headline">¶</a></h2>
<p>The SIFT constructors.</p>
<dl class="function">
<dt id="SIFT::SIFT(int nfeatures, int nOctaveLayers, double contrastThreshold, double edgeThreshold, double sigma)">
<strong>C++:</strong><tt class="descname"> </tt> <tt class="descclassname">SIFT::</tt><tt class="descname">SIFT</tt><big>(</big>int <strong>nfeatures</strong>=0, int <strong>nOctaveLayers</strong>=3, double <strong>contrastThreshold</strong>=0.04, double <strong>edgeThreshold</strong>=10, double <strong>sigma</strong>=1.6<big>)</big><a class="headerlink" href="#SIFT::SIFT(int nfeatures, int nOctaveLayers, double contrastThreshold, double edgeThreshold, double sigma)" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="pyfunction">
<a class="dashAnchor" name="//apple_ref/Function/cv2%2ESIFT"></a><dt id="cv2.SIFT">
<strong>Python:</strong><tt class="descname"> </tt><tt class="descclassname">cv2.</tt><tt class="descname">SIFT</tt><big>(</big><span class="optional">[</span>nfeatures<span class="optional">[</span>, nOctaveLayers<span class="optional">[</span>, contrastThreshold<span class="optional">[</span>, edgeThreshold<span class="optional">[</span>, sigma<span class="optional">]</span><span class="optional">]</span><span class="optional">]</span><span class="optional">]</span><span class="optional">]</span><big>)</big> → &lt;SIFT object&gt;<a class="headerlink" href="#cv2.SIFT" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<colgroup><col class="field-name">
<col class="field-body">
</colgroup><tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>nfeatures</strong> – The number of best features to retain. The features are ranked by their scores (measured in SIFT algorithm as the local contrast)</li>
<li><strong>nOctaveLayers</strong> – The number of layers in each octave. 3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.</li>
<li><strong>contrastThreshold</strong> – The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions. The larger the threshold, the less features are produced by the detector.</li>
<li><strong>edgeThreshold</strong> – The threshold used to filter out edge-like features. Note that the its meaning is different from the contrastThreshold, i.e. the larger the <tt class="docutils literal"><span class="pre">edgeThreshold</span></tt>, the less features are filtered out (more features are retained).</li>
<li><strong>sigma</strong> – The sigma of the Gaussian applied to the input image at the octave #0. If your image is captured with a weak camera with soft lenses, you might want to reduce the number.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="sift-operator">
<h2>SIFT::operator ()<a class="headerlink" href="#sift-operator" title="Permalink to this headline">¶</a></h2>
<p>Extract features and computes their descriptors using SIFT algorithm</p>
<dl class="function">
<dt id="void SIFT::operator()(InputArray img, InputArray mask, vector&lt;KeyPoint&gt;&amp; keypoints, OutputArray descriptors, bool useProvidedKeypoints)">
<strong>C++:</strong><tt class="descname"> </tt>void <tt class="descclassname">SIFT::</tt><tt class="descname">operator()</tt><big>(</big>InputArray <strong>img</strong>, InputArray <strong>mask</strong>, vector&lt;KeyPoint&gt;&amp; <strong>keypoints</strong>, OutputArray <strong>descriptors</strong>, bool <strong>useProvidedKeypoints</strong>=false<big>)</big><a class="headerlink" href="#void SIFT::operator()(InputArray img, InputArray mask, vector&lt;KeyPoint&gt;&amp; keypoints, OutputArray descriptors, bool useProvidedKeypoints)" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="pyfunction">
<a class="dashAnchor" name="//apple_ref/Function/cv2%2ESIFT%2Edetect"></a><dt id="cv2.SIFT.detect">
<strong>Python:</strong><tt class="descname"> </tt><tt class="descclassname">cv2.SIFT.</tt><tt class="descname">detect</tt><big>(</big>image<span class="optional">[</span>, mask<span class="optional">]</span><big>)</big> → keypoints<a class="headerlink" href="#cv2.SIFT.detect" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="pyfunction">
<a class="dashAnchor" name="//apple_ref/Function/cv2%2ESIFT%2Ecompute"></a><dt id="cv2.SIFT.compute">
<strong>Python:</strong><tt class="descname"> </tt><tt class="descclassname">cv2.SIFT.</tt><tt class="descname">compute</tt><big>(</big>image, keypoints<span class="optional">[</span>, descriptors<span class="optional">]</span><big>)</big> → keypoints, descriptors<a class="headerlink" href="#cv2.SIFT.compute" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="pyfunction">
<a class="dashAnchor" name="//apple_ref/Function/cv2%2ESIFT%2EdetectAndCompute"></a><dt id="cv2.SIFT.detectAndCompute">
<strong>Python:</strong><tt class="descname"> </tt><tt class="descclassname">cv2.SIFT.</tt><tt class="descname">detectAndCompute</tt><big>(</big>image, mask<span class="optional">[</span>, descriptors<span class="optional">[</span>, useProvidedKeypoints<span class="optional">]</span><span class="optional">]</span><big>)</big> → keypoints, descriptors<a class="headerlink" href="#cv2.SIFT.detectAndCompute" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<colgroup><col class="field-name">
<col class="field-body">
</colgroup><tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>img</strong> – Input 8-bit grayscale image</li>
<li><strong>mask</strong> – Optional input mask that marks the regions where we should detect features.</li>
<li><strong>keypoints</strong> – The input/output vector of keypoints</li>
<li><strong>descriptors</strong> – The output matrix of descriptors. Pass <tt class="docutils literal"><span class="pre">cv::noArray()</span></tt> if you do not need them.</li>
<li><strong>useProvidedKeypoints</strong> – Boolean flag. If it is true, the keypoint detector is not run. Instead, the provided vector of keypoints is used and the algorithm just computes their descriptors.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Python API provides three functions. First one finds keypoints only. Second function computes the descriptors based on the keypoints we provide. Third function detects the keypoints and computes their descriptors. If you want both keypoints and descriptors, directly use third function as <tt class="docutils literal"><span class="pre">kp,</span> <span class="pre">des</span> <span class="pre">=</span> <span class="pre">cv2.SIFT.detectAndCompute(image,</span> <span class="pre">None)</span></tt></p>
</div>
</div>
<div class="section" id="surf">
<h2>SURF<a class="headerlink" href="#surf" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="SURF : public Feature2D">
<em class="property">class </em><tt class="descname">SURF</tt> : <em class="property">public</em> <tt class="descname">Feature2D</tt><a class="headerlink" href="#SURF : public Feature2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Class for extracting Speeded Up Robust Features from an image <a class="reference internal" href="#bay06" id="id2">[Bay06]</a>. The class is derived from <tt class="docutils literal"><span class="pre">CvSURFParams</span></tt> structure, which specifies the algorithm parameters:</p>
<dl class="member">
<dt id="int extended">
int <tt class="descname">extended</tt><a class="headerlink" href="#int extended" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li>0 means that the basic descriptors (64 elements each) shall be computed</li>
<li>1 means that the extended descriptors (128 elements each) shall be computed</li>
</ul>
</dd></dl>

<dl class="member">
<dt id="int upright">
int <tt class="descname">upright</tt><a class="headerlink" href="#int upright" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li>0 means that detector computes orientation of each feature.</li>
<li>1 means that the orientation is not computed (which is much, much faster). For example, if you match images from a stereo pair, or do image stitching, the matched features likely have very similar angles, and you can speed up feature extraction by setting <tt class="docutils literal"><span class="pre">upright=1</span></tt>.</li>
</ul>
</dd></dl>

<dl class="member">
<dt id="double hessianThreshold">
double <tt class="descname">hessianThreshold</tt><a class="headerlink" href="#double hessianThreshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Threshold for the keypoint detector. Only features, whose hessian is larger than <tt class="docutils literal"><span class="pre">hessianThreshold</span></tt> are retained by the detector. Therefore, the larger the value, the less keypoints you will get. A good default value could be from 300 to 500, depending from the image contrast.</p>
</dd></dl>

<dl class="member">
<dt id="int nOctaves">
int <tt class="descname">nOctaves</tt><a class="headerlink" href="#int nOctaves" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of a gaussian pyramid octaves that the detector uses. It is set to 4 by default. If you want to get very large features, use the larger value. If you want just small features, decrease it.</p>
</dd></dl>

<dl class="member">
<dt id="int nOctaveLayers">
int <tt class="descname">nOctaveLayers</tt><a class="headerlink" href="#int nOctaveLayers" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of images within each octave of a gaussian pyramid. It is set to 2 by default.</p>
</dd></dl>

</dd></dl>

<table class="docutils citation" frame="void" id="bay06" rules="none">
<colgroup><col class="label"><col></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[Bay06]</a></td><td>Bay, H. and Tuytelaars, T. and Van Gool, L. “SURF: Speeded Up Robust Features”, 9th European Conference on Computer Vision, 2006</td></tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li>An example using the SURF feature detector can be found at opencv_source_code/samples/cpp/generic_descriptor_match.cpp</li>
<li>Another example using the SURF feature detector, extractor and matcher can be found at opencv_source_code/samples/cpp/matcher_simple.cpp</li>
</ul>
</div>
</div>
<div class="section" id="surf-surf">
<h2>SURF::SURF<a class="headerlink" href="#surf-surf" title="Permalink to this headline">¶</a></h2>
<p>The SURF extractor constructors.</p>
<dl class="function">
<dt id="SURF::SURF()">
<strong>C++:</strong><tt class="descname"> </tt> <tt class="descclassname">SURF::</tt><tt class="descname">SURF</tt><big>(</big><big>)</big><a class="headerlink" href="#SURF::SURF()" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="SURF::SURF(double hessianThreshold, int nOctaves, int nOctaveLayers, bool extended, bool upright)">
<strong>C++:</strong><tt class="descname"> </tt> <tt class="descclassname">SURF::</tt><tt class="descname">SURF</tt><big>(</big>double <strong>hessianThreshold</strong>, int <strong>nOctaves</strong>=4, int <strong>nOctaveLayers</strong>=2, bool <strong>extended</strong>=true, bool <strong>upright</strong>=false <big>)</big><a class="headerlink" href="#SURF::SURF(double hessianThreshold, int nOctaves, int nOctaveLayers, bool extended, bool upright)" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="pyfunction">
<a class="dashAnchor" name="//apple_ref/Function/cv2%2ESURF"></a><dt id="cv2.SURF">
<strong>Python:</strong><tt class="descname"> </tt><tt class="descclassname">cv2.</tt><tt class="descname">SURF</tt><big>(</big><span class="optional">[</span>hessianThreshold<span class="optional">[</span>, nOctaves<span class="optional">[</span>, nOctaveLayers<span class="optional">[</span>, extended<span class="optional">[</span>, upright<span class="optional">]</span><span class="optional">]</span><span class="optional">]</span><span class="optional">]</span><span class="optional">]</span><big>)</big> → &lt;SURF object&gt;<a class="headerlink" href="#cv2.SURF" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<colgroup><col class="field-name">
<col class="field-body">
</colgroup><tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>hessianThreshold</strong> – Threshold for hessian keypoint detector used in SURF.</li>
<li><strong>nOctaves</strong> – Number of pyramid octaves the keypoint detector will use.</li>
<li><strong>nOctaveLayers</strong> – Number of octave layers within each octave.</li>
<li><strong>extended</strong> – Extended descriptor flag (true - use extended 128-element descriptors; false - use 64-element descriptors).</li>
<li><strong>upright</strong> – Up-right or rotated features flag (true - do not compute orientation of features; false - compute orientation).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="surf-operator">
<h2>SURF::operator()<a class="headerlink" href="#surf-operator" title="Permalink to this headline">¶</a></h2>
<p>Detects keypoints and computes SURF descriptors for them.</p>
<dl class="function">
<dt id="void SURF::operator()(InputArray img, InputArray mask, vector&lt;KeyPoint&gt;&amp; keypoints) const">
<strong>C++:</strong><tt class="descname"> </tt>void <tt class="descclassname">SURF::</tt><tt class="descname">operator()</tt><big>(</big>InputArray <strong>img</strong>, InputArray <strong>mask</strong>, vector&lt;KeyPoint&gt;&amp; <strong>keypoints</strong><big>)</big><tt class="descclassname"> const</tt><a class="headerlink" href="#void SURF::operator()(InputArray img, InputArray mask, vector&lt;KeyPoint&gt;&amp; keypoints) const" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="void SURF::operator()(InputArray img, InputArray mask, vector&lt;KeyPoint&gt;&amp; keypoints, OutputArray descriptors, bool useProvidedKeypoints)">
<strong>C++:</strong><tt class="descname"> </tt>void <tt class="descclassname">SURF::</tt><tt class="descname">operator()</tt><big>(</big>InputArray <strong>img</strong>, InputArray <strong>mask</strong>, vector&lt;KeyPoint&gt;&amp; <strong>keypoints</strong>, OutputArray <strong>descriptors</strong>, bool <strong>useProvidedKeypoints</strong>=false<big>)</big><a class="headerlink" href="#void SURF::operator()(InputArray img, InputArray mask, vector&lt;KeyPoint&gt;&amp; keypoints, OutputArray descriptors, bool useProvidedKeypoints)" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="pyfunction">
<a class="dashAnchor" name="//apple_ref/Function/cv2%2ESURF%2Edetect"></a><dt id="cv2.SURF.detect">
<strong>Python:</strong><tt class="descname"> </tt><tt class="descclassname">cv2.SURF.</tt><tt class="descname">detect</tt><big>(</big>image<span class="optional">[</span>, mask<span class="optional">]</span><big>)</big> → keypoints<a class="headerlink" href="#cv2.SURF.detect" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="pyfunction">
<a class="dashAnchor" name="//apple_ref/Function/cv2%2ESURF%2Ecompute"></a><dt id="cv2.SURF.compute">
<strong>Python:</strong><tt class="descname"> </tt><tt class="descclassname">cv2.SURF.</tt><tt class="descname">compute</tt><big>(</big>image, keypoints<span class="optional">[</span>, descriptors<span class="optional">]</span><big>)</big> → keypoints, descriptors<a class="headerlink" href="#cv2.SURF.compute" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="pyfunction">
<a class="dashAnchor" name="//apple_ref/Function/cv2%2ESURF%2EdetectAndCompute"></a><dt id="cv2.SURF.detectAndCompute">
<strong>Python:</strong><tt class="descname"> </tt><tt class="descclassname">cv2.SURF.</tt><tt class="descname">detectAndCompute</tt><big>(</big>image, mask<span class="optional">[</span>, descriptors<span class="optional">[</span>, useProvidedKeypoints<span class="optional">]</span><span class="optional">]</span><big>)</big> → keypoints, descriptors<a class="headerlink" href="#cv2.SURF.detectAndCompute" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="pyfunction">
<dt>
<strong>Python:</strong><tt class="descname"> </tt><tt class="descclassname">cv2.SURF.</tt><tt class="descname">detectAndCompute</tt><big>(</big>image<span class="optional">[</span>, mask<span class="optional">]</span><big>)</big> → keypoints, descriptors</dt>
<dd></dd></dl>

<dl class="cfunction">
<dt id="void cvExtractSURF(const CvArr* image, const CvArr* mask, CvSeq** keypoints, CvSeq** descriptors, CvMemStorage* storage, CvSURFParams params)">
<strong>C:</strong><tt class="descname"> </tt>void <tt class="descname">cvExtractSURF</tt><big>(</big>const CvArr* <strong>image</strong>, const CvArr* <strong>mask</strong>, CvSeq** <strong>keypoints</strong>, CvSeq** <strong>descriptors</strong>, CvMemStorage* <strong>storage</strong>, CvSURFParams <strong>params</strong><big>)</big><a class="headerlink" href="#void cvExtractSURF(const CvArr* image, const CvArr* mask, CvSeq** keypoints, CvSeq** descriptors, CvMemStorage* storage, CvSURFParams params)" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<colgroup><col class="field-name">
<col class="field-body">
</colgroup><tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>image</strong> – Input 8-bit grayscale image</li>
<li><strong>mask</strong> – Optional input mask that marks the regions where we should detect features.</li>
<li><strong>keypoints</strong> – The input/output vector of keypoints</li>
<li><strong>descriptors</strong> – The output matrix of descriptors. Pass <tt class="docutils literal"><span class="pre">cv::noArray()</span></tt> if you do not need them.</li>
<li><strong>useProvidedKeypoints</strong> – Boolean flag. If it is true, the keypoint detector is not run. Instead, the provided vector of keypoints is used and the algorithm just computes their descriptors.</li>
<li><strong>storage</strong> – Memory storage for the output keypoints and descriptors in OpenCV 1.x API.</li>
<li><strong>params</strong> – SURF algorithm parameters in OpenCV 1.x API.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<p>The function is parallelized with the TBB library.</p>
<p>If you are using the C version, make sure you call <tt class="docutils literal"><span class="pre">cv::initModule_xfeatures2d()</span></tt> from <tt class="docutils literal"><span class="pre">xfeatures2d/nonfree.hpp</span></tt>.</p>
</div>
<div class="section" id="cuda-surf-cuda">
<h2>cuda::SURF_CUDA<a class="headerlink" href="#cuda-surf-cuda" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cuda::SURF_CUDA">
<em class="property">class </em><tt class="descclassname">cuda::</tt><tt class="descname">SURF_CUDA</tt><a class="headerlink" href="#cuda::SURF_CUDA" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Class used for extracting Speeded Up Robust Features (SURF) from an image.</p>
<div class="highlight-python"><div class="highlight"><pre>class SURF_CUDA
{
public:
    enum KeypointLayout
    {
        X_ROW = 0,
        Y_ROW,
        LAPLACIAN_ROW,
        OCTAVE_ROW,
        SIZE_ROW,
        ANGLE_ROW,
        HESSIAN_ROW,
        ROWS_COUNT
    };

    //! the default constructor
    SURF_CUDA();
    //! the full constructor taking all the necessary parameters
    explicit SURF_CUDA(double _hessianThreshold, int _nOctaves=4,
         int _nOctaveLayers=2, bool _extended=false, float _keypointsRatio=0.01f);

    //! returns the descriptor size in float's (64 or 128)
    int descriptorSize() const;

    //! upload host keypoints to device memory
    void uploadKeypoints(const vector&lt;KeyPoint&gt;&amp; keypoints,
        GpuMat&amp; keypointsGPU);
    //! download keypoints from device to host memory
    void downloadKeypoints(const GpuMat&amp; keypointsGPU,
        vector&lt;KeyPoint&gt;&amp; keypoints);

    //! download descriptors from device to host memory
    void downloadDescriptors(const GpuMat&amp; descriptorsGPU,
        vector&lt;float&gt;&amp; descriptors);

    void operator()(const GpuMat&amp; img, const GpuMat&amp; mask,
        GpuMat&amp; keypoints);

    void operator()(const GpuMat&amp; img, const GpuMat&amp; mask,
        GpuMat&amp; keypoints, GpuMat&amp; descriptors,
        bool useProvidedKeypoints = false,
        bool calcOrientation = true);

    void operator()(const GpuMat&amp; img, const GpuMat&amp; mask,
        std::vector&lt;KeyPoint&gt;&amp; keypoints);

    void operator()(const GpuMat&amp; img, const GpuMat&amp; mask,
        std::vector&lt;KeyPoint&gt;&amp; keypoints, GpuMat&amp; descriptors,
        bool useProvidedKeypoints = false,
        bool calcOrientation = true);

    void operator()(const GpuMat&amp; img, const GpuMat&amp; mask,
        std::vector&lt;KeyPoint&gt;&amp; keypoints,
        std::vector&lt;float&gt;&amp; descriptors,
        bool useProvidedKeypoints = false,
        bool calcOrientation = true);

    void releaseMemory();

    // SURF parameters
    double hessianThreshold;
    int nOctaves;
    int nOctaveLayers;
    bool extended;
    bool upright;

    //! max keypoints = keypointsRatio * img.size().area()
    float keypointsRatio;

    GpuMat sum, mask1, maskSum, intBuffer;

    GpuMat det, trace;

    GpuMat maxPosBuffer;
};
</pre></div>
</div>
<p>The class <tt class="docutils literal"><span class="pre">SURF_CUDA</span></tt> implements Speeded Up Robust Features descriptor. There is a fast multi-scale Hessian keypoint detector that can be used to find the keypoints (which is the default option). But the descriptors can also be computed for the user-specified keypoints. Only 8-bit grayscale images are supported.</p>
<p>The class <tt class="docutils literal"><span class="pre">SURF_CUDA</span></tt> can store results in the GPU and CPU memory. It provides functions to convert results between CPU and GPU version ( <tt class="docutils literal"><span class="pre">uploadKeypoints</span></tt>, <tt class="docutils literal"><span class="pre">downloadKeypoints</span></tt>, <tt class="docutils literal"><span class="pre">downloadDescriptors</span></tt> ). The format of CPU results is the same as <tt class="docutils literal"><span class="pre">SURF</span></tt> results. GPU results are stored in <tt class="docutils literal"><span class="pre">GpuMat</span></tt>. The <tt class="docutils literal"><span class="pre">keypoints</span></tt> matrix is <img class="math" src="../../../_images/math/8751b70edb8d3b203d842bc56667a5526dcdfab1.png" alt="\texttt{nFeatures} \times 7"> matrix with the <tt class="docutils literal"><span class="pre">CV_32FC1</span></tt> type.</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">keypoints.ptr&lt;float&gt;(X_ROW)[i]</span></tt> contains x coordinate of the i-th feature.</li>
<li><tt class="docutils literal"><span class="pre">keypoints.ptr&lt;float&gt;(Y_ROW)[i]</span></tt> contains y coordinate of the i-th feature.</li>
<li><tt class="docutils literal"><span class="pre">keypoints.ptr&lt;float&gt;(LAPLACIAN_ROW)[i]</span></tt>  contains the laplacian sign of the i-th feature.</li>
<li><tt class="docutils literal"><span class="pre">keypoints.ptr&lt;float&gt;(OCTAVE_ROW)[i]</span></tt> contains the octave of the i-th feature.</li>
<li><tt class="docutils literal"><span class="pre">keypoints.ptr&lt;float&gt;(SIZE_ROW)[i]</span></tt> contains the size of the i-th feature.</li>
<li><tt class="docutils literal"><span class="pre">keypoints.ptr&lt;float&gt;(ANGLE_ROW)[i]</span></tt> contain orientation of the i-th feature.</li>
<li><tt class="docutils literal"><span class="pre">keypoints.ptr&lt;float&gt;(HESSIAN_ROW)[i]</span></tt> contains the response of the i-th feature.</li>
</ul>
<p>The <tt class="docutils literal"><span class="pre">descriptors</span></tt> matrix is <img class="math" src="../../../_images/math/93325a9a1014934ae52792d8ccc1347b3f79828a.png" alt="\texttt{nFeatures} \times \texttt{descriptorSize}"> matrix with the <tt class="docutils literal"><span class="pre">CV_32FC1</span></tt> type.</p>
<p>The class <tt class="docutils literal"><span class="pre">SURF_CUDA</span></tt> uses some buffers and provides access to it. All buffers can be safely released between function calls.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#SURF : public Feature2D" title="class SURF : public Feature2D"><tt class="xref ocv ocv-class docutils literal"><span class="pre">SURF</span></tt></a></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li>An example for using the SURF keypoint matcher on GPU can be found at opencv_source_code/samples/gpu/surf_keypoint_matcher.cpp</li>
</ul>
</div>
</div>
</div>


          </div>
          <div class="feedback">
              <h2>Help and Feedback</h2>
              You did not find what you were looking for?
              <ul>
                  
                  
                  
                  <li>Ask a question on the <a href="http://answers.opencv.org/">Q&amp;A forum</a>.</li>
                  <li>If you think something is missing or wrong in the documentation,
                  please file a <a href="http://code.opencv.org/">bug report</a>.</li>
              </ul>
          </div>
        </div>
      </div>

      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index">index</a></li>
        <li class="right">
          <a href="../../ximgproc/doc/ximgproc.html" title="ximgproc. Extended Image Processing">next</a> |</li>
        <li class="right">
          <a href="extra_features.html" title="Experimental 2D Features Algorithms">previous</a> |</li>
        <li><a href="../../../index.html">OpenCV 3.0.0-dev documentation</a> »</li>
          <li><a href="../../refman.html">OpenCV API Reference</a> »</li>
          <li><a href="xfeatures2d.html">xfeatures2d. Extra 2D Features Framework</a> »</li> 
      </ul>
    </div>
    <div class="footer">
        © Copyright 2011-2014, opencv dev team.
      Last updated on Dec 30, 2014.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.2.
      <a href="../../../_sources/modules/xfeatures2d/doc/nonfree_features.txt" rel="nofollow">Show this page source.</a>
    </div>
  

<!-- Mirrored from docs.opencv.org/3.0-last-rst/modules/xfeatures2d/doc/nonfree_features.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 23 Dec 2015 06:59:19 GMT -->
</body></html>